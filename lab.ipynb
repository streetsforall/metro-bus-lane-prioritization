{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.speedups\n",
    "from branca.colormap import LinearColormap\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from shapely.geometry import LineString, Polygon\n",
    "from shapely.ops import substring, unary_union\n",
    "\n",
    "if shapely.speedups.available:\n",
    "    shapely.speedups.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pref = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON files\n",
    "midday_gdf = gpd.read_file(path_pref + \"182_Midday_speeds.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function cleans the short names in the CalTrans speed input file we use for sequence matching, as there are problematic non-numeric characters\n",
    "def process_route_name(value):\n",
    "    if pd.isna(value):\n",
    "        print(\"Encountered a null value in route_short_name.\")\n",
    "        return value\n",
    "\n",
    "    value = str(value)  # Convert to string to handle potential float values\n",
    "\n",
    "    if \"/\" in value:\n",
    "        value = value.split(\"/\")[0]\n",
    "\n",
    "    value = value.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "    if value == \"\":\n",
    "        print(\"Encountered an empty string in route_short_name after processing.\")\n",
    "        return value\n",
    "\n",
    "    try:\n",
    "        int(value)\n",
    "        # return value\n",
    "    except ValueError:\n",
    "        print(f\"Value '{value}' in route_short_name is not convertible to an integer.\")\n",
    "        # return value\n",
    "    return value\n",
    "\n",
    "\n",
    "# Apply the function to the 'route_short_name' column\n",
    "midday_gdf[\"route_short_name\"] = midday_gdf[\"route_short_name\"].apply(\n",
    "    process_route_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique identifier\n",
    "midday_gdf[\"stop_route_short_dir_id\"] = (\n",
    "    midday_gdf[\"stop_id\"].astype(str)\n",
    "    + \"_\"\n",
    "    + midday_gdf[\"route_short_name\"].astype(str)\n",
    "    + \"_\"\n",
    "    + midday_gdf[\"direction_id\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GeoJSON LineString CalTrans Speed Data, save CRS, select relevant columns\n",
    "full = gpd.read_file(path_pref + \"metro_lines.geojson\")\n",
    "original_crs = full.crs\n",
    "full_cols = full[\n",
    "    [\n",
    "        \"route_id\",\n",
    "        \"direction_id\",\n",
    "        \"stop_pair\",\n",
    "        \"stop_pair_name\",\n",
    "        \"segment_id\",\n",
    "        \"time_of_day\",\n",
    "        \"p50_mph\",\n",
    "        \"p80_mph\",\n",
    "        \"route_short_name\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates, keep the one with the highest difference in speed\n",
    "full_cols[\"speed_diff\"] = full_cols[\"p80_mph\"] - full_cols[\"p50_mph\"]\n",
    "full_cols = full_cols[full_cols[\"time_of_day\"] != \"Owl\"]\n",
    "\n",
    "\n",
    "# Group by 'stop_route_dir_id' and get the index of the max 'speed_diff'\n",
    "max_speed_diff_indices = full_cols.groupby(\n",
    "    [\"route_short_name\", \"segment_id\", \"time_of_day\"]\n",
    ")[\"speed_diff\"].idxmax()  # route short name?\n",
    "\n",
    "# Create a new GeoDataFrame with the rows corresponding to the max indices for each duplicate group\n",
    "cleaned_gdf = full_cols.loc[max_speed_diff_indices].reset_index(drop=True)\n",
    "cleaned_gdf = cleaned_gdf.drop(columns=[\"speed_diff\"])\n",
    "cleaned_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_gdf[cleaned_gdf[\"stop_pair_name\"] == \"5th / San Pedro__5th / Main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_gdf[\n",
    "    cleaned_gdf[\"stop_pair_name\"] == \"Santa Monica / 3rd St Promenade__5th / Colorado\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'segment_id' and aggregate\n",
    "merged_gdf = cleaned_gdf.groupby(\n",
    "    [\"segment_id\", \"route_short_name\"], as_index=False\n",
    ").agg(\n",
    "    {\n",
    "        \"route_id\": \"first\",  # Take the first value for route_id\n",
    "        \"direction_id\": \"first\",  # Take the first value for direction_id\n",
    "        \"stop_pair\": \"first\",  # Take the first value for stop_pair\n",
    "        \"stop_pair_name\": \"first\",  # Take the first value for stop_pair_name\n",
    "        #'route_short_name': 'first',  # Take the first value for route_short_name\n",
    "        \"geometry\": \"first\",  # Take the first geometry\n",
    "        \"p80_mph\": \"max\",  # Take the maximum p80_mph\n",
    "        #'p50_mph': 'mean',  # Take the mean of p50_mph TODO: Which values to use?\n",
    "        \"p50_mph\": lambda x: (\n",
    "            x[\n",
    "                cleaned_gdf.loc[x.index, \"time_of_day\"].isin(\n",
    "                    [\"AM Peak\", \"PM Peak\", \"Midday\"]\n",
    "                )\n",
    "            ].mean()\n",
    "            or x.mean()\n",
    "        )\n",
    "        or 0.1,  # Fallback to overall mean if filtered mean is zero\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the resulting GeoDataFrame\n",
    "print(\"Aggregated GeoDataFrame:\")\n",
    "merged_gdf = gpd.GeoDataFrame(\n",
    "    merged_gdf, geometry=\"geometry\", crs=original_crs\n",
    ")  # Replace \"geometry\" with your column name\n",
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify there are no zero values in the 50th percentile speed for aggregated segments\n",
    "print(\n",
    "    \"Number of segments with zero values for p50 speed:\",\n",
    "    len(merged_gdf[merged_gdf[\"p50_mph\"] == 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf[merged_gdf[\"stop_pair_name\"] == \"5th / San Pedro__5th / Main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf[merged_gdf[\"segment_id\"] == \"8066-8028-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out segments for routes beginning with 8 and 9, which are non-bus routes\n",
    "merged_gdf = merged_gdf[~merged_gdf[\"route_id\"].str.startswith(\"8\")]\n",
    "merged_gdf = merged_gdf[~merged_gdf[\"route_id\"].str.startswith(\"9\")]\n",
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the short_name cleaning function to the 'route_short_name' column\n",
    "merged_gdf[\"route_short_name\"] = merged_gdf[\"route_short_name\"].apply(\n",
    "    process_route_name\n",
    ")\n",
    "merged_gdf[\"route_short_name\"] = merged_gdf[\"route_short_name\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf[\"stop_id\"] = merged_gdf[\"stop_pair\"].str.split(\"_\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Ridership file\n",
    "ridership_full = pd.read_csv(path_pref + \"bus_ridership.csv\")\n",
    "ridership_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the non-weekday data\n",
    "ridership_full = ridership_full[ridership_full[\"DAY TYPE\"] == \"DX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridership_full[\"DIRECTION\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean, process, and map ridership data values\n",
    "ridership_full[\"YEAR MONTH\"] = ridership_full[\"YEAR MONTH\"].astype(str)\n",
    "ridership_full[\"LINE\"] = ridership_full[\"LINE\"].astype(str)\n",
    "ridership_full[\"DIRECTION_bin\"] = ridership_full[\"DIRECTION\"].map(\n",
    "    {\n",
    "        \"North\": 0.0,\n",
    "        \"South\": 1.0,\n",
    "        \"East\": 0.0,\n",
    "        \"West\": 1.0,\n",
    "        \"Clockwise\": 0.0,\n",
    "        \"Counterclockwise\": 1.0,\n",
    "    }\n",
    ")\n",
    "ridership_full[\"STOP ID\"] = ridership_full[\"STOP ID\"].astype(str)\n",
    "\n",
    "# Create a unique identifier\n",
    "ridership_full[\"stop_route_short_dir_id\"] = (\n",
    "    ridership_full[\"STOP ID\"]\n",
    "    + \"_\"\n",
    "    + ridership_full[\"LINE\"]\n",
    "    + \"_\"\n",
    "    + ridership_full[\"DIRECTION_bin\"].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same identifier in merged_gdf\n",
    "merged_gdf[\"stop_route_short_dir_id\"] = (\n",
    "    merged_gdf[\"stop_id\"].astype(str)\n",
    "    + \"_\"\n",
    "    + merged_gdf[\"route_short_name\"].astype(str)\n",
    "    + \"_\"\n",
    "    + merged_gdf[\"direction_id\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up NaN, non-numeric, and missing values in ridership data\n",
    "ridership_full[\"Avg Ons\"] = pd.to_numeric(\n",
    "    ridership_full[\"Avg Ons\"], errors=\"coerce\"\n",
    ").fillna(0)\n",
    "ridership_full[\"Avg Offs\"] = pd.to_numeric(\n",
    "    ridership_full[\"Avg Offs\"], errors=\"coerce\"\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate ridership data to handle duplicates\n",
    "ridership_agg = (\n",
    "    ridership_full.groupby(\"stop_route_short_dir_id\")\n",
    "    .agg({\"Avg Ons\": \"mean\", \"Avg Offs\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "print(ridership_agg)\n",
    "\n",
    "# Create a dictionary for efficient mapping\n",
    "ridership_dict = ridership_agg.set_index(\"stop_route_short_dir_id\")[\n",
    "    [\"Avg Ons\", \"Avg Offs\"]\n",
    "].to_dict(orient=\"index\")\n",
    "\n",
    "# Use map to add Avg Ons and Avg Offs to merged_gdf\n",
    "merged_gdf[\"Avg Ons\"] = merged_gdf[\"stop_route_short_dir_id\"].map(\n",
    "    lambda x: ridership_dict.get(x, {}).get(\"Avg Ons\")\n",
    ")\n",
    "merged_gdf[\"Avg Offs\"] = merged_gdf[\"stop_route_short_dir_id\"].map(\n",
    "    lambda x: ridership_dict.get(x, {}).get(\"Avg Offs\")\n",
    ")\n",
    "\n",
    "\n",
    "# Check if the number of rows remained the same\n",
    "print(f\"Original merged_gdf rows: {len(merged_gdf)}\")\n",
    "print(f\"Final merged data rows: {len(merged_gdf)}\")\n",
    "\n",
    "# Check for any null values in the new columns\n",
    "print(f\"Null values in merged Avg Ons: {merged_gdf['Avg Ons'].isnull().sum()}\")\n",
    "print(f\"Null values in merged Avg Offs: {merged_gdf['Avg Offs'].isnull().sum()}\")\n",
    "\n",
    "# Check for unique values in stop_route_short_dir_id\n",
    "print(\n",
    "    f\"Unique stop_route_short_dir_id in merged_gdf: {merged_gdf['stop_route_short_dir_id'].nunique()}\"\n",
    ")\n",
    "print(\n",
    "    f\"Unique stop_route_short_dir_id in ridership_full: {ridership_full['stop_route_short_dir_id'].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sequence data to handle duplicates\n",
    "sequence_agg = (\n",
    "    midday_gdf.groupby(\"stop_route_short_dir_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"stop_sequence\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create a dictionary for efficient mapping\n",
    "sequence_dict = sequence_agg.set_index(\"stop_route_short_dir_id\")[\n",
    "    [\"stop_sequence\"]\n",
    "].to_dict(orient=\"index\")\n",
    "\n",
    "# Use map to add sequence ids to merged_gdf\n",
    "merged_gdf[\"stop_sequence\"] = merged_gdf[\"stop_route_short_dir_id\"].map(\n",
    "    lambda x: sequence_dict.get(x, {}).get(\"stop_sequence\")\n",
    ")\n",
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing stop sequence values\n",
    "merged_gdf[\n",
    "    ~merged_gdf[\"stop_sequence\"].apply(\n",
    "        lambda x: isinstance(x, (int, float)) and not np.isnan(x)\n",
    "    )\n",
    "][\n",
    "    [\n",
    "        \"stop_sequence\",\n",
    "        \"stop_pair_name\",\n",
    "        \"segment_id\",\n",
    "        \"route_short_name\",\n",
    "        \"stop_pair\",\n",
    "        \"Avg Ons\",\n",
    "        \"Avg Offs\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block aims to fill missing sequence data. \n",
    "The stop_pair_names are split and used to fill those missing values iteratively \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fill_missing_stop_sequences(merged_gdf):\n",
    "    # First, parse out the stop IDs from the stop_pair column\n",
    "    merged_gdf[[\"first_stop_id\", \"second_stop_id\"]] = merged_gdf[\"stop_pair\"].str.split(\n",
    "        \"__\", expand=True\n",
    "    )\n",
    "\n",
    "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    df = merged_gdf.copy()\n",
    "\n",
    "    # Find rows with missing stop_sequence\n",
    "    missing_sequence_mask = df[\"stop_sequence\"].isna()\n",
    "    print(\"Initial missing sequences:\", sum(missing_sequence_mask))\n",
    "\n",
    "    # Iterate through rows with missing stop_sequence\n",
    "    for idx in df[missing_sequence_mask].index:\n",
    "        # Get route and direction for the current row\n",
    "        route_short_name = df.loc[idx, \"route_short_name\"]\n",
    "        direction_id = df.loc[idx, \"direction_id\"]\n",
    "        first_stop_id = df.loc[idx, \"first_stop_id\"]\n",
    "        second_stop_id = df.loc[idx, \"second_stop_id\"]\n",
    "\n",
    "        # Strategy 1: Find rows where first_stop matches second_stop of missing row\n",
    "        # and the previous row has a known sequence\n",
    "        matching_rows_forward = df[\n",
    "            (df[\"route_short_name\"] == route_short_name)\n",
    "            & (df[\"direction_id\"] == direction_id)\n",
    "            & (df[\"stop_sequence\"].notna())\n",
    "            & (df[\"second_stop_id\"] == first_stop_id)\n",
    "        ]\n",
    "\n",
    "        # Strategy 2: Find rows where second_stop matches first_stop of missing row\n",
    "        # and the matching row has a known sequence\n",
    "        matching_rows_backward = df[\n",
    "            (df[\"route_short_name\"] == route_short_name)\n",
    "            & (df[\"direction_id\"] == direction_id)\n",
    "            & (df[\"stop_sequence\"].notna())\n",
    "            & (df[\"first_stop_id\"] == second_stop_id)\n",
    "        ]\n",
    "\n",
    "        # If forward matching rows exist, take the sequence and add 1\n",
    "        if len(matching_rows_forward) > 0:\n",
    "            df.loc[idx, \"stop_sequence\"] = (\n",
    "                matching_rows_forward[\"stop_sequence\"].iloc[0] + 1\n",
    "            )\n",
    "\n",
    "        # If backward matching rows exist, take the sequence and subtract 1\n",
    "        elif len(matching_rows_backward) > 0:\n",
    "            df.loc[idx, \"stop_sequence\"] = (\n",
    "                matching_rows_backward[\"stop_sequence\"].iloc[0] - 1\n",
    "            )\n",
    "\n",
    "    # Count remaining missing sequences\n",
    "    remaining_missing = df[\"stop_sequence\"].isna().sum()\n",
    "    return df, remaining_missing\n",
    "\n",
    "\n",
    "def iteratively_fill_stop_sequences(merged_gdf):\n",
    "    \"\"\"\n",
    "    Repeatedly run fill_missing_stop_sequences until no more sequences can be filled.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    merged_gdf : pandas.DataFrame\n",
    "        Input DataFrame with transit stop information\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with as many stop sequences filled as possible\n",
    "    \"\"\"\n",
    "    current_df = merged_gdf.copy()\n",
    "    previous_missing_count = float(\"inf\")\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        current_df, missing_count = fill_missing_stop_sequences(current_df)\n",
    "\n",
    "        print(f\"Iteration {iteration}: {missing_count} sequences still missing\")\n",
    "\n",
    "        # If no progress is made, break the loop\n",
    "        if missing_count == previous_missing_count:\n",
    "            break\n",
    "\n",
    "        previous_missing_count = missing_count\n",
    "\n",
    "    print(\n",
    "        f\"Stopped after {iteration} iterations with {missing_count} missing sequences\"\n",
    "    )\n",
    "    return current_df\n",
    "\n",
    "\n",
    "final_df = iteratively_fill_stop_sequences(merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates the ridership curve across the route segments by direction in sequence order. \n",
    "The \"Ons\" and \"Offs\" are intuitively aggregated so that we have a useful curve of the daily ridership for each route-segment-direction tuple.\n",
    "One caveat is that the CalTrans data splits long segments into parts, so for segments that are created from the split we do not modify the current rider count\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def calculate_route_ridership(data, route, direction):\n",
    "    # Filter the dataframe for the specific route and direction\n",
    "    route_df = data[\n",
    "        (data[\"route_short_name\"] == str(route)) & (data[\"direction_id\"] == direction)\n",
    "    ].sort_values(\"stop_sequence\")\n",
    "\n",
    "    print(route_df)\n",
    "    # Initialize variables\n",
    "    riders_on_bus = 0\n",
    "    segment_ridership = {}\n",
    "\n",
    "    print(f\"\\nCalculating ridership for route {route}, direction {direction}\")\n",
    "\n",
    "    for _, row in route_df.iterrows():\n",
    "        print(row[\"stop_sequence\"])\n",
    "        stop_id = int(row[\"stop_id\"] or \"0\")\n",
    "        stop_name = row[\"stop_pair_name\"]\n",
    "        stop_route_short_dir_id = row[\"stop_route_short_dir_id\"]\n",
    "\n",
    "        # Check if segment_id ends with '1', meaning it is not a split segment in the CalTrans data\n",
    "        ends_with_one = str(row[\"segment_id\"]).endswith(\"1\")\n",
    "\n",
    "        # Assign values based on conditions\n",
    "        riders_getting_on = (\n",
    "            row[\"Avg Ons\"] if pd.notna(row[\"Avg Ons\"]) and ends_with_one else 0\n",
    "        )\n",
    "        riders_getting_off = (\n",
    "            row[\"Avg Offs\"] if pd.notna(row[\"Avg Offs\"]) and ends_with_one else 0\n",
    "        )\n",
    "\n",
    "        # Update riders on bus\n",
    "        riders_on_bus += riders_getting_on - riders_getting_off\n",
    "        riders_on_bus = max(0, riders_on_bus)  # Ensure it doesn't go below zero\n",
    "\n",
    "        segment_ridership[stop_route_short_dir_id] = riders_on_bus\n",
    "\n",
    "        print(\n",
    "            f\"Stop {stop_id} - {stop_name}: On: {riders_getting_on:.2f}, Off: {riders_getting_off:.2f}, On Bus: {riders_on_bus:.2f}\"\n",
    "        )\n",
    "\n",
    "    return segment_ridership\n",
    "\n",
    "\n",
    "# Test the function with route \"720\" in direction 0\n",
    "test_route = \"720\"\n",
    "test_direction = 0\n",
    "result = calculate_route_ridership(merged_gdf, test_route, test_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the ridership calculation for every route for both directions\n",
    "def calculate_all_routes_ridership(merged_data):\n",
    "    all_segment_ridership = {}\n",
    "    # Get unique combinations of route_short_name and direction_id\n",
    "    route_directions = merged_data[\n",
    "        [\"route_short_name\", \"direction_id\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    for _, row in route_directions.iterrows():\n",
    "        route = row[\"route_short_name\"]\n",
    "        direction = row[\"direction_id\"]\n",
    "        print(f\"Calculating ridership for route {route}, direction {direction}\")\n",
    "        segment_ridership = calculate_route_ridership(merged_data, route, direction)\n",
    "        all_segment_ridership.update(segment_ridership)\n",
    "\n",
    "    return all_segment_ridership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ridership for all routes\n",
    "all_routes_ridership = calculate_all_routes_ridership(merged_gdf)\n",
    "\n",
    "# Add the calculated ridership to the original DataFrame\n",
    "merged_gdf[\"calculated_segment_ridership\"] = merged_gdf[\"stop_route_short_dir_id\"].map(\n",
    "    all_routes_ridership\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a segment shared by two bus routes, the 2 and the 4, to see the distinct ridership count\n",
    "merged_gdf[merged_gdf[\"segment_id\"] == \"8066-8028-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the ridership curves across the network with an interactive folium map\n",
    "\"\"\"\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = merged_gdf[\"calculated_segment_ridership\"].min()\n",
    "vmax = merged_gdf[\"calculated_segment_ridership\"].max()\n",
    "color_map = LinearColormap(colors=[\"blue\", \"yellow\", \"red\"], vmin=vmin, vmax=vmax)\n",
    "\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature[\"properties\"][\"calculated_segment_ridership\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = \"gray\"  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n",
    "\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature[\"properties\"]\n",
    "    ridership = props[\"calculated_segment_ridership\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_pair_name']}<br>\"\n",
    "        f\"Route: {props['route_short_name']}<br>\"\n",
    "        f\"Ridership: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Add the GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    merged_gdf,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"stop_pair_name\", \"route_short_name\", \"calculated_segment_ridership\"],\n",
    "        aliases=[\"Stop Name:\", \"Route:\", \"Ridership:\"],\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = \"\"\"\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "             \"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "# m.save(\"los_angeles_bus_ridership_map.html\")\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoDataFrame to a GeoJSON-like Python dictionary\n",
    "geojson_dict = merged_gdf.__geo_interface__\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"bus_segments_calculated_ridership.geojson\", \"w\") as f:\n",
    "    json.dump(geojson_dict, f)\n",
    "\n",
    "print(\"Data saved successfully to bus_segments_calculated_ridership.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delay_effects(gdf):\n",
    "    \"\"\"\n",
    "    Calculate delay effects for the consolidated segments using average p50 speed across time periods.\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame has a valid CRS\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame must have a CRS set.\")\n",
    "\n",
    "    # Reproject to a projected CRS for accurate length calculations (e.g., EPSG:3857 for meters)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Calculate length of LineString geometries in meters\n",
    "    gdf[\"length_meters\"] = gdf.geometry.length\n",
    "\n",
    "    # Convert length to miles (1 meter = 0.000621371 miles)\n",
    "    gdf[\"miles_from_last\"] = gdf[\"length_meters\"] * 0.000621371\n",
    "\n",
    "    # Calculate difference from average speed\n",
    "    gdf[\"diff_from_avg\"] = gdf[\"p80_mph\"] - gdf[\"p50_mph\"]\n",
    "\n",
    "    # Calculate actual and best travel times (hours)\n",
    "    gdf[\"actual_time\"] = gdf[\"miles_from_last\"] / gdf[\"p50_mph\"]\n",
    "    gdf[\"best_time\"] = gdf[\"miles_from_last\"] / np.maximum(\n",
    "        gdf[\"p80_mph\"], 1.25 * gdf[\"p50_mph\"]\n",
    "    )\n",
    "\n",
    "    # Calculate delay in hours and time lost in minutes\n",
    "    gdf[\"delay_hours\"] = np.maximum(0, gdf[\"actual_time\"] - gdf[\"best_time\"])\n",
    "    gdf[\"time_lost\"] = gdf[\"delay_hours\"] * 60\n",
    "\n",
    "    # Calculate total ridership minutes lost\n",
    "    gdf[\"total_ridership_minutes_lost\"] = (\n",
    "        gdf[\"time_lost\"] * gdf[\"calculated_segment_ridership\"]\n",
    "    )\n",
    "\n",
    "    # Reproject back to original CRS if necessary\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = calculate_delay_effects(merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_to_merge = merged_gdf[\n",
    "    [\n",
    "        \"stop_id\",\n",
    "        \"stop_pair_name\",\n",
    "        \"route_id\",\n",
    "        \"route_short_name\",\n",
    "        \"direction_id\",\n",
    "        \"miles_from_last\",\n",
    "        \"geometry\",\n",
    "        \"stop_sequence\",\n",
    "        \"stop_route_short_dir_id\",\n",
    "        \"calculated_segment_ridership\",\n",
    "        \"p50_mph\",\n",
    "        \"p80_mph\",\n",
    "        \"diff_from_avg\",\n",
    "        \"time_lost\",\n",
    "        \"total_ridership_minutes_lost\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify data types before merging segments to appropriately preserve and/or aggregate features\n",
    "gdf_to_merge[\"direction_id\"] = gdf_to_merge[\"direction_id\"].astype(str)\n",
    "gdf_to_merge[\"stop_sequence\"] = gdf_to_merge[\"stop_sequence\"].astype(str)\n",
    "gdf_to_merge[\"p50_mph\"] = gdf_to_merge[\"p50_mph\"].astype(str)\n",
    "gdf_to_merge[\"p80_mph\"] = gdf_to_merge[\"p80_mph\"].astype(str)\n",
    "gdf_to_merge[\"diff_from_avg\"] = gdf_to_merge[\"diff_from_avg\"].astype(str)\n",
    "gdf_to_merge[\"calculated_segment_ridership_str\"] = gdf_to_merge[\n",
    "    \"calculated_segment_ridership\"\n",
    "].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/cal-itp/data-analyses/blob/f62b150768fb1547c6b604cb53d122531104d099/_shared_utils/shared_utils/rt_utils.py#L587-L632\n",
    "def try_parallel(geometry):\n",
    "    try:\n",
    "        return geometry.parallel_offset(30, \"right\")\n",
    "    except Exception:\n",
    "        return geometry\n",
    "\n",
    "\n",
    "def extend_line(line, extension_length):\n",
    "    \"\"\"Extend a line by adding length to both ends.\"\"\"\n",
    "    if not isinstance(line, LineString):\n",
    "        return line\n",
    "\n",
    "    coords = list(line.coords)\n",
    "    if len(coords) < 2:\n",
    "        return line\n",
    "\n",
    "    # Get the direction vector at the start of the line\n",
    "    start_vec = np.array(coords[1]) - np.array(coords[0])\n",
    "    start_vec = start_vec / np.linalg.norm(start_vec) * extension_length\n",
    "    new_start = np.array(coords[0]) - start_vec\n",
    "\n",
    "    # Get the direction vector at the end of the line\n",
    "    end_vec = np.array(coords[-1]) - np.array(coords[-2])\n",
    "    end_vec = end_vec / np.linalg.norm(end_vec) * extension_length\n",
    "    new_end = np.array(coords[-1]) + end_vec\n",
    "\n",
    "    # Create new coordinates list\n",
    "    new_coords = [new_start] + coords + [new_end]\n",
    "    return LineString(new_coords)\n",
    "\n",
    "\n",
    "def arrowize_segment(\n",
    "    line_geometry, buffer_distance: int = 20, extension_length: int = 10\n",
    "):\n",
    "    \"\"\"Given a linestring segment from a GTFS shape,\n",
    "    buffer and clip to show direction of progression.\n",
    "\n",
    "    Additionally, creates an extended version of the segment before buffering.\n",
    "    \"\"\"\n",
    "    arrow_distance = buffer_distance  # was buffer_distance * 0.75\n",
    "    st_end_distance = arrow_distance + 3  # avoid floating point errors...\n",
    "\n",
    "    try:\n",
    "        segment = line_geometry.simplify(tolerance=5)\n",
    "        arrow_distance = max(arrow_distance, line_geometry.length / 20)\n",
    "        shift_distance = buffer_distance + 1\n",
    "\n",
    "        # Create extended version of the original segment\n",
    "        extended_segment = extend_line(segment, extension_length)\n",
    "\n",
    "        # Create the arrowized version\n",
    "        begin_segment = substring(segment, 0, st_end_distance)\n",
    "        r_shift = begin_segment.parallel_offset(shift_distance, \"right\")\n",
    "        r_pt = substring(r_shift, 0, 0)\n",
    "        l_shift = begin_segment.parallel_offset(shift_distance, \"left\")\n",
    "        l_pt = substring(l_shift, 0, 0)\n",
    "        end = substring(\n",
    "            begin_segment,\n",
    "            begin_segment.length + arrow_distance,\n",
    "            begin_segment.length + arrow_distance,\n",
    "        )\n",
    "        poly = shapely.geometry.Polygon(\n",
    "            (r_pt, end, l_pt)\n",
    "        )  # Triangle to cut bottom of arrow\n",
    "\n",
    "        end_segment = substring(\n",
    "            segment, segment.length - st_end_distance, segment.length\n",
    "        )\n",
    "        end = substring(end_segment, end_segment.length, end_segment.length)\n",
    "        r_shift = end_segment.parallel_offset(shift_distance, \"right\")\n",
    "        r_pt = substring(r_shift, 0, 0)\n",
    "        r_pt2 = substring(r_shift, r_shift.length, r_shift.length)\n",
    "        l_shift = end_segment.parallel_offset(shift_distance, \"left\")\n",
    "        l_pt = substring(l_shift, 0, 0)\n",
    "        l_pt2 = substring(l_shift, l_shift.length, l_shift.length)\n",
    "        t1 = shapely.geometry.Polygon((l_pt2, end, l_pt))\n",
    "        t2 = shapely.geometry.Polygon((r_pt2, end, r_pt))\n",
    "\n",
    "        segment_clip_mask = shapely.geometry.MultiPolygon((poly, t1, t2))\n",
    "\n",
    "        # Buffer and clip segment with arrow shape\n",
    "        differences = segment.buffer(buffer_distance).difference(segment_clip_mask)\n",
    "\n",
    "        # Pick the largest resulting geometry\n",
    "        final_geometry = max(differences.geoms, key=lambda x: x.area)\n",
    "\n",
    "        # 2. Extended version (just buffered, no cuts)\n",
    "        extended = extended_segment.buffer(buffer_distance, cap_style=2)\n",
    "\n",
    "        return final_geometry, extended\n",
    "\n",
    "    except Exception:\n",
    "        return line_geometry.simplify(tolerance=5).buffer(\n",
    "            buffer_distance\n",
    "        ), line_geometry.buffer(buffer_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to programmatically merge LineStrings, we buffer them into polygons and shorten them. This later allows us to merge based on overlap.\n",
    "# Note: approach changed away form this method, preserving it for now\n",
    "def modify_line_geometry(gdf, reduce_length=0.1, width=0.001):\n",
    "    \"\"\"\n",
    "    Modify line geometries in a GeoDataFrame by:\n",
    "    1. Shortening lines from both ends\n",
    "    2. Converting lines to thin polygons with specified width.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame with LineString geometries.\n",
    "    reduce_length : float, optional (default=0.1)\n",
    "        Fraction of line length to remove from each end.\n",
    "    width : float, optional (default=0.001)\n",
    "        Width of the resulting polygon in coordinate units.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with modified polygon geometries.\n",
    "    \"\"\"\n",
    "\n",
    "    def modify_line(line):\n",
    "        # Calculate the total line length\n",
    "        total_length = line.length\n",
    "\n",
    "        # Compute reduction distance\n",
    "        reduce_dist = total_length * reduce_length\n",
    "\n",
    "        # Ensure we only shorten if the line is long enough\n",
    "        if total_length > 2 * reduce_dist:\n",
    "            # Shorten the line by removing `reduce_dist` from each end\n",
    "            shortened_line = substring(line, reduce_dist, total_length - reduce_dist)\n",
    "        else:\n",
    "            # If the line is too short, skip shortening\n",
    "            shortened_line = line\n",
    "\n",
    "        # Buffer the line to create a thin polygon\n",
    "        return shortened_line.buffer(width, cap_style=2)  # cap_style=2 for square ends\n",
    "\n",
    "    # Create a copy of the GeoDataFrame to avoid modifying the original\n",
    "    modified_gdf = gdf.copy()\n",
    "\n",
    "    # Apply the geometry modification\n",
    "    modified_gdf.geometry = modified_gdf.geometry.apply(modify_line)\n",
    "\n",
    "    return modified_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the adapted Cal-ITP arrowize method for clearer looking segments post-merge\n",
    "print(gdf_to_merge.crs)\n",
    "# Ensure the dataset has a CRS\n",
    "if gdf_to_merge.crs is None:\n",
    "    gdf_to_merge.set_crs(epsg=4326, inplace=True)  # Assume WGS84 if no CRS is set\n",
    "\n",
    "# Convert to a projected CRS for proper geometric operations\n",
    "projected_crs = \"EPSG:32611\"  # UTM Zone 11N (meters)\n",
    "gdf_to_merge = gdf_to_merge.to_crs(projected_crs)\n",
    "\n",
    "gdf_to_merge.geometry = gdf_to_merge.geometry.apply(try_parallel)\n",
    "\n",
    "gdf_to_merge[[\"geometry\", \"extended_geometry\"]] = gdf_to_merge[\"geometry\"].apply(\n",
    "    lambda geom: pd.Series(\n",
    "        arrowize_segment(geom, buffer_distance=20, extension_length=10)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by direction so merging is distinct to direction of flow\n",
    "gdf_to_merge_dir_zero = gdf_to_merge[gdf_to_merge[\"direction_id\"] == \"0.0\"]\n",
    "gdf_to_merge_dir_one = gdf_to_merge[gdf_to_merge[\"direction_id\"] == \"1.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_crs_with_extended_geom(gdf, target_crs):\n",
    "    \"\"\"\n",
    "    Safely converts both primary and extended geometries to the target CRS.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame with 'geometry' and 'extended_geometry' columns\n",
    "    target_crs: Target coordinate reference system (can be EPSG code or proj string)\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: New GeoDataFrame with both geometries converted to target CRS\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = gdf.copy()\n",
    "\n",
    "    # Store the extended geometry\n",
    "    extended = result[\"extended_geometry\"]\n",
    "\n",
    "    # First convert the main GeoDataFrame (this converts the primary geometry)\n",
    "    result = result.to_crs(target_crs)\n",
    "\n",
    "    # Create a temporary GeoDataFrame with extended_geometry as the primary geometry\n",
    "    temp_gdf = gpd.GeoDataFrame(geometry=extended, crs=gdf.crs)\n",
    "\n",
    "    # Convert the extended geometries\n",
    "    converted_extended = temp_gdf.to_crs(target_crs).geometry\n",
    "\n",
    "    # Put the converted extended geometry back\n",
    "    result[\"extended_geometry\"] = converted_extended\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Convert to geographic CRS (EPSG:4326) for visualization\n",
    "gdf_to_merge_dir_zero = convert_crs_with_extended_geom(\n",
    "    gdf_to_merge_dir_zero, target_crs=4326\n",
    ")\n",
    "gdf_to_merge_dir_one = convert_crs_with_extended_geom(\n",
    "    gdf_to_merge_dir_one, target_crs=4326\n",
    ")\n",
    "\n",
    "# Check lengths in decimal degrees\n",
    "print(\"Lengths in decimal degrees (Direction 0):\")\n",
    "print(gdf_to_merge_dir_zero.geometry.length.describe())\n",
    "print(\"\\nExtended geometry lengths in decimal degrees (Direction 0):\")\n",
    "print(gdf_to_merge_dir_zero.extended_geometry.length.describe())\n",
    "\n",
    "print(\"\\nLengths in decimal degrees (Direction 1):\")\n",
    "print(gdf_to_merge_dir_one.geometry.length.describe())\n",
    "print(\"\\nExtended geometry lengths in decimal degrees (Direction 1):\")\n",
    "print(gdf_to_merge_dir_one.extended_geometry.length.describe())\n",
    "\n",
    "# Convert back to UTM for accurate length measurements\n",
    "gdf_to_merge_dir_zero_utm = convert_crs_with_extended_geom(\n",
    "    gdf_to_merge_dir_zero, projected_crs\n",
    ")\n",
    "gdf_to_merge_dir_one_utm = convert_crs_with_extended_geom(\n",
    "    gdf_to_merge_dir_one, projected_crs\n",
    ")\n",
    "\n",
    "print(\"\\nLengths in meters (Direction 0):\")\n",
    "print(gdf_to_merge_dir_zero_utm.geometry.length.describe())\n",
    "print(\"\\nExtended geometry lengths in meters (Direction 0):\")\n",
    "print(gdf_to_merge_dir_zero_utm.extended_geometry.length.describe())\n",
    "\n",
    "print(\"\\nLengths in meters (Direction 1):\")\n",
    "print(gdf_to_merge_dir_one_utm.geometry.length.describe())\n",
    "print(\"\\nExtended geometry lengths in meters (Direction 1):\")\n",
    "print(gdf_to_merge_dir_one_utm.extended_geometry.length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geom_ov_fast(gdf, overlap_threshold=0.8, debug=True):\n",
    "    \"\"\"\n",
    "    Optimized version of merge_geom_ov using spatial indexing and sparse matrices.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame with polygon geometries\n",
    "    overlap_threshold (float): Minimum overlap ratio required for merging (0.0 to 1.0)\n",
    "    debug (bool): Whether to print debug messages\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: Processed GeoDataFrame with selectively merged segments\n",
    "    \"\"\"\n",
    "\n",
    "    def log_debug(message):\n",
    "        if debug:\n",
    "            print(message)\n",
    "\n",
    "    def create_safe_extended_geometry(regular_geom, original_ext_geoms):\n",
    "        \"\"\"\n",
    "        Creates a valid extended geometry that properly contains the regular geometry.\n",
    "        Uses original extended geometries as reference for the buffer distance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate typical buffer distance from original geometries\n",
    "            buffer_distances = []\n",
    "            for reg, ext in zip(group_data.geometry, group_data.extended_geometry):\n",
    "                if reg is not None and ext is not None:\n",
    "                    # Find minimum distance that would make regular geometry fit in extended\n",
    "                    min_dist = max(0, reg.distance(ext.boundary))\n",
    "                    buffer_distances.append(min_dist)\n",
    "\n",
    "            if buffer_distances:\n",
    "                # Use median buffer distance to avoid outliers\n",
    "                typical_buffer = np.median(buffer_distances)\n",
    "                # Add a small safety margin\n",
    "                buffer_distance = typical_buffer * 1.1\n",
    "            else:\n",
    "                # Fallback: use a proportion of the regular geometry's size\n",
    "                buffer_distance = max(\n",
    "                    regular_geom.length * 0.1, regular_geom.area**0.5 * 0.05\n",
    "                )\n",
    "\n",
    "            extended_geom = regular_geom.buffer(buffer_distance)\n",
    "\n",
    "            # Verify the result\n",
    "            if not extended_geom.contains(regular_geom):\n",
    "                log_debug(\n",
    "                    \"Warning: Created extended geometry doesn't contain regular geometry. Increasing buffer.\"\n",
    "                )\n",
    "                extended_geom = regular_geom.buffer(buffer_distance * 1.5)\n",
    "\n",
    "            return extended_geom\n",
    "\n",
    "        except Exception as e:\n",
    "            log_debug(f\"Error creating safe extended geometry: {str(e)}\")\n",
    "            # Ultimate fallback: simple buffer\n",
    "            return regular_geom.buffer(regular_geom.area**0.5 * 0.1)\n",
    "\n",
    "    try:\n",
    "        log_debug(\"Starting optimized segment merging...\")\n",
    "\n",
    "        log_debug(\"Type: \" + str(type(gdf)))\n",
    "\n",
    "        # Instead of checking is_valid on the Series, we'll check individual geometries\n",
    "        gdf = gdf.copy()  # Make a copy to avoid modifying the original\n",
    "\n",
    "        # Fix any invalid geometries\n",
    "        for idx, row in gdf.iterrows():\n",
    "            try:\n",
    "                if not row.geometry.is_valid:\n",
    "                    gdf.at[idx, \"geometry\"] = row.geometry.buffer(0)\n",
    "                if not row.extended_geometry.is_valid:\n",
    "                    gdf.at[idx, \"extended_geometry\"] = row.extended_geometry.buffer(0)\n",
    "            except Exception as e:\n",
    "                log_debug(f\"Warning: Error fixing geometry at index {idx}: {str(e)}\")\n",
    "\n",
    "        # Create spatial index\n",
    "        spatial_index = gdf.sindex\n",
    "\n",
    "        # Pre-compute areas for all polygons\n",
    "        areas = np.array([geom.area for geom in gdf.geometry])\n",
    "\n",
    "        # Initialize sparse matrix for connectivity\n",
    "        n = len(gdf)\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "\n",
    "        # Function to check overlap ratio\n",
    "        def check_overlap(idx1, idx2):\n",
    "            if idx1 == idx2:\n",
    "                return False\n",
    "\n",
    "            geom1 = gdf.geometry.iloc[idx1]\n",
    "            geom2 = gdf.geometry.iloc[idx2]\n",
    "\n",
    "            if not (isinstance(geom1, Polygon) and isinstance(geom2, Polygon)):\n",
    "                return False\n",
    "\n",
    "            intersection_area = geom1.intersection(geom2).area\n",
    "            min_area = min(areas[idx1], areas[idx2])\n",
    "\n",
    "            if min_area == 0:\n",
    "                return False\n",
    "\n",
    "            return (intersection_area / min_area) >= overlap_threshold\n",
    "\n",
    "        # Use spatial index to find potential overlaps\n",
    "        for idx in range(n):\n",
    "            geom = gdf.geometry.iloc[idx]\n",
    "            bounds = geom.bounds\n",
    "\n",
    "            # Get potential neighbors using spatial index\n",
    "            potential_matches_idx = list(spatial_index.intersection(bounds))\n",
    "\n",
    "            # Filter actual overlaps\n",
    "            for match_idx in potential_matches_idx:\n",
    "                if match_idx <= idx:  # Only check each pair once\n",
    "                    continue\n",
    "\n",
    "                if check_overlap(idx, match_idx):\n",
    "                    rows.extend([idx, match_idx])\n",
    "                    cols.extend([match_idx, idx])\n",
    "                    data.extend([1, 1])\n",
    "\n",
    "        # Create sparse matrix\n",
    "        adjacency_matrix = csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "\n",
    "        # Find connected components\n",
    "        n_components, labels = connected_components(adjacency_matrix, directed=False)\n",
    "\n",
    "        log_debug(f\"Found {n_components} distinct groups\")\n",
    "\n",
    "        # Create new GeoDataFrame with merged geometries\n",
    "        new_rows = []\n",
    "\n",
    "        for component_id in range(n_components):\n",
    "            # Get indices for this component\n",
    "            component_indices = np.where(labels == component_id)[0]\n",
    "\n",
    "            if len(component_indices) == 1:\n",
    "                # Single segment, no merging needed\n",
    "                new_rows.append(gdf.iloc[component_indices[0]])\n",
    "            else:\n",
    "                # Get the segments to merge\n",
    "                group_data = gdf.iloc[component_indices]\n",
    "\n",
    "                # Merge regular geometries\n",
    "                merged_geom = unary_union(group_data.geometry)\n",
    "                if not merged_geom.is_valid:\n",
    "                    merged_geom = merged_geom.buffer(0)\n",
    "\n",
    "                # Try merging extended geometries\n",
    "                try:\n",
    "                    merged_geom_ext = unary_union(group_data.extended_geometry)\n",
    "                    if not merged_geom_ext.is_valid:\n",
    "                        merged_geom_ext = merged_geom_ext.buffer(0)\n",
    "\n",
    "                    # Check if merged extended geometry is valid and contains regular geometry\n",
    "                    if merged_geom_ext.is_empty or not merged_geom_ext.contains(\n",
    "                        merged_geom\n",
    "                    ):\n",
    "                        if merged_geom_ext.is_empty:\n",
    "                            print(\"EMPTY\")\n",
    "                        log_debug(\n",
    "                            f\"Component {component_id}: Extended geometry invalid or doesn't contain regular geometry\"\n",
    "                        )\n",
    "                        log_debug(\n",
    "                            f\"Extended geometry area: {merged_geom_ext.area}, Regular geometry area: {merged_geom.area}\"\n",
    "                        )\n",
    "                        merged_geom_ext = create_safe_extended_geometry(\n",
    "                            merged_geom, group_data.extended_geometry\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    log_debug(\n",
    "                        f\"Extended geometry merge failed for component {component_id}: {str(e)}\"\n",
    "                    )\n",
    "                    merged_geom_ext = create_safe_extended_geometry(\n",
    "                        merged_geom, group_data.extended_geometry\n",
    "                    )\n",
    "\n",
    "                # Aggregate non-geometric data\n",
    "                agg_data = {}\n",
    "                for col in group_data.columns:\n",
    "                    if col == \"geometry\":\n",
    "                        continue\n",
    "                    elif col == \"miles_from_last\":\n",
    "                        agg_data[col] = group_data[col].max()\n",
    "                    elif pd.api.types.is_numeric_dtype(group_data[col]):\n",
    "                        agg_data[col] = group_data[col].sum()\n",
    "                    else:\n",
    "                        values = group_data[col].dropna().unique()\n",
    "                        agg_data[col] = \", \".join(str(v) for v in values)\n",
    "\n",
    "                agg_data[\"geometry\"] = merged_geom\n",
    "                agg_data[\"extended_geometry\"] = merged_geom_ext\n",
    "                new_rows.append(pd.Series(agg_data))\n",
    "\n",
    "        # Create final GeoDataFrame\n",
    "        merged_gdf = gpd.GeoDataFrame(new_rows, crs=gdf.crs)\n",
    "        log_debug(f\"Final segment count: {len(merged_gdf)}\")\n",
    "\n",
    "        return merged_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        log_debug(f\"Error during merging: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the ridership curves across the network with an interactive folium map\n",
    "\"\"\"\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = gdf_to_merge_dir_zero[\"total_ridership_minutes_lost\"].min()\n",
    "vmax = gdf_to_merge_dir_zero[\"total_ridership_minutes_lost\"].max()\n",
    "color_map = LinearColormap(colors=[\"blue\", \"yellow\", \"red\"], vmin=vmin, vmax=vmax)\n",
    "\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature[\"properties\"][\"total_ridership_minutes_lost\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = \"gray\"  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n",
    "\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature[\"properties\"]\n",
    "    ridership = props[\"calculated_segment_ridership\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_pair_name']}<br>\"\n",
    "        f\"Route: {props['route_short_name']}<br>\"\n",
    "        f\"Ridership: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Add the GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    gdf_to_merge_dir_zero,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"stop_pair_name\", \"route_short_name\", \"calculated_segment_ridership\"],\n",
    "        aliases=[\"Stop Name:\", \"Route:\", \"Ridership:\"],\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = \"\"\"\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "             \"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "# m.save(\"los_angeles_bus_ridership_map.html\")\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the merge code which included the currently out-of-date extended geometry.\n",
    "try:\n",
    "    log_debug = lambda msg: print(f\"DEBUG: {msg}\")\n",
    "\n",
    "    log_debug(f\"Processing first dataset (size: {len(gdf_to_merge_dir_zero)})\")\n",
    "    processed = merge_geom_ov_fast(gdf_to_merge_dir_zero)\n",
    "\n",
    "    log_debug(f\"Processing second dataset (size: {len(gdf_to_merge_dir_one)})\")\n",
    "    processed_one = merge_geom_ov_fast(gdf_to_merge_dir_one)\n",
    "\n",
    "    if processed is None or processed_one is None:\n",
    "        print(\"Processing failed - check the error messages above\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Successfully processed both datasets: {len(processed)} and {len(processed_one)} rows\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the aggregated minutes lost normalized by the segment length\n",
    "processed[\"minutes_lost_per_mile\"] = (\n",
    "    processed[\"total_ridership_minutes_lost\"] / processed[\"miles_from_last\"]\n",
    ")\n",
    "processed_one[\"minutes_lost_per_mile\"] = (\n",
    "    processed_one[\"total_ridership_minutes_lost\"] / processed_one[\"miles_from_last\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the generation for now, but not using it in the current corridor process\n",
    "processed = processed.drop(columns=[\"extended_geometry\"])\n",
    "processed_one = processed_one.drop(columns=[\"extended_geometry\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive network map colored by total ridership-minutes lost\n",
    "\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Modify color map creation\n",
    "color_map = LinearColormap(\n",
    "    colors=[\"blue\", \"yellow\", \"red\"],\n",
    "    vmin=min(vmin, vmax),  # Ensure correct order\n",
    "    vmax=max(vmin, vmax),\n",
    ")\n",
    "\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature[\"properties\"][\"total_ridership_minutes_lost\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = \"gray\"  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n",
    "\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature[\"properties\"]\n",
    "    ridership = props[\"total_ridership_minutes_lost\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_name']}<br>\"\n",
    "        f\"Route: {props['route_id']}<br>\"\n",
    "        f\"Minutes Lost: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define fields and aliases for tooltip (excluding 'geometry')\n",
    "tooltip_fields = [\n",
    "    \"stop_id\",\n",
    "    \"stop_pair_name\",\n",
    "    \"route_id\",\n",
    "    \"direction_id\",\n",
    "    \"miles_from_last\",\n",
    "    \"calculated_segment_ridership_str\",\n",
    "    \"calculated_segment_ridership\",\n",
    "    \"p50_mph\",\n",
    "    \"diff_from_avg\",\n",
    "    \"time_lost\",\n",
    "    \"total_ridership_minutes_lost\",\n",
    "]\n",
    "\n",
    "tooltip_aliases = [\n",
    "    \"Stop ID:\",\n",
    "    \"Stop Name:\",\n",
    "    \"Route ID:\",\n",
    "    \"Direction:\",\n",
    "    \"Miles From Last:\",\n",
    "    \"Segment Ridership:\",\n",
    "    \"Segment Ridership Aggregated:\",\n",
    "    \"Average Speed (mph):\",\n",
    "    \"Speed Difference:\",\n",
    "    \"Time Lost (min):\",\n",
    "    \"Total Minutes Lost:\",\n",
    "]\n",
    "\n",
    "# Add the first GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "# Add the second GeoDataFrame to the map\n",
    "# folium.GeoJson(\n",
    "#     processed_one,\n",
    "#     style_function=style_function,\n",
    "#     popup=popup_function,\n",
    "#     tooltip=folium.GeoJsonTooltip(\n",
    "#         fields=tooltip_fields,\n",
    "#         aliases=tooltip_aliases,\n",
    "#         localize=True,\n",
    "#         sticky=False,\n",
    "#         labels=True,\n",
    "#         style=\"\"\"\n",
    "#             background-color: #F0EFEF;\n",
    "#             border: 2px solid black;\n",
    "#             border-radius: 3px;\n",
    "#             box-shadow: 3px;\n",
    "#             padding: 10px;\n",
    "#             font-size: 12px;\n",
    "#         \"\"\",\n",
    "#         max_width=800,\n",
    "#     ),\n",
    "# ).add_to(m)\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = \"\"\"\n",
    "    <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "# m.save('los_angeles_bus_map.html')\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive network map colored by total ridership-minutes lost per mile\n",
    "\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = processed[\"minutes_lost_per_mile\"].min()\n",
    "vmax = processed[\"minutes_lost_per_mile\"].max()\n",
    "color_map = LinearColormap(colors=[\"blue\", \"yellow\", \"red\"], vmin=vmin, vmax=vmax)\n",
    "\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature[\"properties\"][\"minutes_lost_per_mile\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = \"gray\"  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n",
    "\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature[\"properties\"]\n",
    "    ridership = props[\"minutes_lost_per_mile\"]\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_name']}<br>\"\n",
    "        f\"Route: {props['route_id']}<br>\"\n",
    "        f\"Minutes Lost: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define fields and aliases for tooltip (excluding 'geometry')\n",
    "tooltip_fields = [\n",
    "    \"stop_id\",\n",
    "    \"stop_pair_name\",\n",
    "    \"route_id\",\n",
    "    \"direction_id\",\n",
    "    \"miles_from_last\",\n",
    "    \"calculated_segment_ridership_str\",\n",
    "    \"calculated_segment_ridership\",\n",
    "    \"p50_mph\",\n",
    "    \"diff_from_avg\",\n",
    "    \"time_lost\",\n",
    "    \"total_ridership_minutes_lost\",\n",
    "]\n",
    "\n",
    "tooltip_aliases = [\n",
    "    \"Stop ID:\",\n",
    "    \"Stop Name:\",\n",
    "    \"Route ID:\",\n",
    "    \"Direction:\",\n",
    "    \"Miles From Last:\",\n",
    "    \"Segment Ridership:\",\n",
    "    \"Segment Ridership Aggregated:\",\n",
    "    \"Average Speed (mph):\",\n",
    "    \"Speed Difference:\",\n",
    "    \"Time Lost (min):\",\n",
    "    \"Total Minutes Lost:\",\n",
    "]\n",
    "\n",
    "# Add the first GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "# Add the second GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed_one,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = \"\"\"\n",
    "    <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "# m.save('los_angeles_bus_map.html')\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 segments by rider-delay for the direction '1'\n",
    "processed_one.sort_values(\"total_ridership_minutes_lost\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total ridership minutes lost across all segments\n",
    "total_minutes = (\n",
    "    processed[\"total_ridership_minutes_lost\"].sum()\n",
    "    + processed_one[\"total_ridership_minutes_lost\"].sum()\n",
    ")\n",
    "\n",
    "# Convert to hours for better readability\n",
    "total_hours = total_minutes / 60\n",
    "\n",
    "per_pass = total_minutes / 770000\n",
    "\n",
    "print(f\"Total passenger minutes lost: {total_minutes:,.2f}\")\n",
    "print(f\"Total passenger hours lost: {total_hours:,.2f}\")\n",
    "print(f\"Total minutes per passenger lost: {per_pass:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is our overall corridor ID process. Aim is to group together consecutive segments with high levels of delay per mile into clear corridor groups.\n",
    "We do this by first filtering for a certain percentile threshold to get the high impact segments, then iterating across those segments in groups of streets so we only group segments running along the same street.\n",
    "For each street we do a breadth first search to add segments until there is a break. Then we evaluate that grouping for whether it is significant. For us, this criteria is just whether total length is at least 1 mile.    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_stop_info(stop_pair_name):\n",
    "    \"\"\"\n",
    "    Extract detailed stop information from stop_pair_name.\n",
    "    Returns a list of tuples (primary_street, from_stop, to_stop) for each pair.\n",
    "    \"\"\"\n",
    "    stop_pair_name = str(stop_pair_name)\n",
    "    pairs = stop_pair_name.split(\",\")\n",
    "    stop_info = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        stops = pair.strip().split(\"__\")\n",
    "        if len(stops) == 2:\n",
    "            from_stop = stops[0].strip()\n",
    "            to_stop = stops[1].strip()\n",
    "            # Extract primary street (before slash) from first stop\n",
    "            primary_street = from_stop.split(\"/\")[0].strip()\n",
    "            stop_info.append((primary_street, from_stop, to_stop))\n",
    "\n",
    "    return stop_info\n",
    "\n",
    "\n",
    "def extract_primary_streets(stop_pair_name):\n",
    "    \"\"\"\n",
    "    Extract unique primary street names from stop_pair_name.\n",
    "    \"\"\"\n",
    "    stop_info = extract_stop_info(stop_pair_name)\n",
    "    return list(set(info[0] for info in stop_info))\n",
    "\n",
    "\n",
    "def are_segments_continuous(segment1_name, segment2_name):\n",
    "    \"\"\"\n",
    "    Check if two segments are continuous by comparing their stop names.\n",
    "    Returns True if any stop is shared between the segments.\n",
    "    \"\"\"\n",
    "    seg1_info = extract_stop_info(segment1_name)\n",
    "    seg2_info = extract_stop_info(segment2_name)\n",
    "\n",
    "    # Get all stops from both segments\n",
    "    seg1_stops = set()\n",
    "    seg2_stops = set()\n",
    "\n",
    "    for _, from_stop, to_stop in seg1_info:\n",
    "        seg1_stops.add(from_stop)\n",
    "        seg1_stops.add(to_stop)\n",
    "\n",
    "    for _, from_stop, to_stop in seg2_info:\n",
    "        seg2_stops.add(from_stop)\n",
    "        seg2_stops.add(to_stop)\n",
    "\n",
    "    # Check if segments share any stops\n",
    "    return bool(seg1_stops.intersection(seg2_stops))\n",
    "\n",
    "\n",
    "def find_continuous_adjacent_segments(segment_name, candidates_df):\n",
    "    \"\"\"\n",
    "    Find adjacent segments that are continuously connected based on shared stops.\n",
    "    \"\"\"\n",
    "    adjacent_segments = []\n",
    "\n",
    "    for idx, row in candidates_df.iterrows():\n",
    "        if are_segments_continuous(segment_name, row[\"stop_pair_name\"]):\n",
    "            adjacent_segments.append(idx)\n",
    "\n",
    "    return candidates_df.loc[adjacent_segments]\n",
    "\n",
    "\n",
    "def process_corridors_by_street_continuous(\n",
    "    processed_df, threshold_value, threshold_percentile=75\n",
    "):\n",
    "    \"\"\"\n",
    "    Process corridors street by street ensuring continuous segments.\n",
    "    \"\"\"\n",
    "    # Clean up any existing index columns from previous runs\n",
    "    df = processed_df.copy()\n",
    "    cols_to_drop = [col for col in df.columns if col.startswith(\"index_\")]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    if \"index\" in df.columns:\n",
    "        df = df.drop(columns=[\"index\"])\n",
    "\n",
    "    column_name = f\"corridor_group_id_{threshold_percentile}\"\n",
    "\n",
    "    # Rest of the code remains the same...\n",
    "    df[\"primary_streets\"] = df[\"stop_pair_name\"].apply(extract_primary_streets)\n",
    "    all_streets = set()\n",
    "    for streets in df[\"primary_streets\"]:\n",
    "        all_streets.update(streets)\n",
    "    threshold = threshold_value  # df[\"minutes_lost_per_mile\"].quantile(threshold_percentile / 100)\n",
    "\n",
    "    corridor_groups = []\n",
    "    global_group_id = 1\n",
    "\n",
    "    for street in sorted(all_streets):\n",
    "        street_mask = df[\"primary_streets\"].apply(lambda x: street in x)\n",
    "        street_df = df[street_mask].copy()\n",
    "        high_impact_df = street_df[\n",
    "            street_df[\"minutes_lost_per_mile\"] >= threshold\n",
    "        ].copy()\n",
    "\n",
    "        if high_impact_df.empty:\n",
    "            continue\n",
    "\n",
    "        visited = set()\n",
    "        for idx, row in high_impact_df.iterrows():\n",
    "            if idx in visited:\n",
    "                continue\n",
    "\n",
    "            current_group = [idx]\n",
    "            current_length = row[\"miles_from_last\"]\n",
    "            queue = [idx]\n",
    "            visited.add(idx)\n",
    "\n",
    "            while queue:\n",
    "                current_segment_idx = queue.pop()\n",
    "                current_segment = high_impact_df.loc[current_segment_idx]\n",
    "                neighbors = find_continuous_adjacent_segments(\n",
    "                    current_segment[\"stop_pair_name\"],\n",
    "                    high_impact_df[~high_impact_df.index.isin(visited)],\n",
    "                )\n",
    "\n",
    "                for neighbor_idx, neighbor in neighbors.iterrows():\n",
    "                    if neighbor_idx not in visited:\n",
    "                        queue.append(neighbor_idx)\n",
    "                        current_group.append(neighbor_idx)\n",
    "                        current_length += neighbor[\"miles_from_last\"]\n",
    "                        visited.add(neighbor_idx)\n",
    "\n",
    "            if current_length >= 1:\n",
    "                corridor_groups.extend(\n",
    "                    [(idx, global_group_id, street) for idx in current_group]\n",
    "                )\n",
    "                global_group_id += 1\n",
    "\n",
    "    # Create results DataFrame and merge carefully\n",
    "    group_df = pd.DataFrame(\n",
    "        corridor_groups, columns=[\"index\", column_name, \"primary_street\"]\n",
    "    )\n",
    "    result = df.reset_index().merge(group_df, on=\"index\", how=\"left\")\n",
    "    result[column_name] = result[column_name].fillna(0).astype(int)\n",
    "    result = result.set_index(\"index\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next few cells run the corridor ID code for different threshold values and merge together.\n",
    "combined_for_threshold = pd.concat([processed, processed_one])\n",
    "threshold_value_85 = combined_for_threshold[\"minutes_lost_per_mile\"].quantile(0.85)\n",
    "threshold_value_90 = combined_for_threshold[\"minutes_lost_per_mile\"].quantile(0.90)\n",
    "threshold_value_95 = combined_for_threshold[\"minutes_lost_per_mile\"].quantile(0.95)\n",
    "print(threshold_value_85, threshold_value_90, threshold_value_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_approach_90 = process_corridors_by_street_continuous(\n",
    "    processed, threshold_value_90, 90\n",
    ")\n",
    "street_approach_one_90 = process_corridors_by_street_continuous(\n",
    "    processed_one, threshold_value_90, 90\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_approach_85 = process_corridors_by_street_continuous(\n",
    "    processed, threshold_value_85, 85\n",
    ")\n",
    "street_approach_one_85 = process_corridors_by_street_continuous(\n",
    "    processed_one, threshold_value_85, 85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_approach_95 = process_corridors_by_street_continuous(\n",
    "    processed, threshold_value_95, 95\n",
    ")\n",
    "street_approach_one_95 = process_corridors_by_street_continuous(\n",
    "    processed_one, threshold_value_95, 95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for all dataframes to avoid duplicate index issues\n",
    "dir1_base = processed.reset_index()\n",
    "dir1_85 = street_approach_85.reset_index().rename(columns={\"index\": \"original_index\"})\n",
    "dir1_90 = street_approach_90.reset_index().rename(columns={\"index\": \"original_index\"})\n",
    "dir1_95 = street_approach_95.reset_index().rename(columns={\"index\": \"original_index\"})\n",
    "\n",
    "dir2_base = processed_one.reset_index()\n",
    "dir2_85 = street_approach_one_85.reset_index().rename(\n",
    "    columns={\"index\": \"original_index\"}\n",
    ")\n",
    "dir2_90 = street_approach_one_90.reset_index().rename(\n",
    "    columns={\"index\": \"original_index\"}\n",
    ")\n",
    "dir2_95 = street_approach_one_95.reset_index().rename(\n",
    "    columns={\"index\": \"original_index\"}\n",
    ")\n",
    "\n",
    "# Merge the first direction datasets using the explicit merge function\n",
    "dir1_merged = dir1_base.merge(\n",
    "    dir1_85[[\"original_index\", \"corridor_group_id_85\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "dir1_merged = dir1_merged.merge(\n",
    "    dir1_90[[\"original_index\", \"corridor_group_id_90\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "dir1_merged = dir1_merged.merge(\n",
    "    dir1_95[[\"original_index\", \"corridor_group_id_95\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "# Merge the second direction datasets\n",
    "dir2_merged = dir2_base.merge(\n",
    "    dir2_85[[\"original_index\", \"corridor_group_id_85\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "dir2_merged = dir2_merged.merge(\n",
    "    dir2_90[[\"original_index\", \"corridor_group_id_90\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "dir2_merged = dir2_merged.merge(\n",
    "    dir2_95[[\"original_index\", \"corridor_group_id_95\"]],\n",
    "    left_on=\"index\",\n",
    "    right_on=\"original_index\",\n",
    "    how=\"left\",\n",
    ").drop(\"original_index\", axis=1)\n",
    "\n",
    "# Combine both directions\n",
    "combined_results = pd.concat([dir1_merged, dir2_merged], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "from branca.colormap import linear\n",
    "\n",
    "# Load your GeoDataFrame (assuming it's already loaded as gdf)\n",
    "gdf = combined_results[\n",
    "    combined_results[\"corridor_group_id_90\"] != 0\n",
    "]  # Replace with your actual GeoDataFrame variable\n",
    "\n",
    "\n",
    "# Compute the center of the map\n",
    "map_center = [gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()]\n",
    "\n",
    "# Create a folium map\n",
    "m = folium.Map(location=map_center, zoom_start=11, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Define a color scale based on corridor_group_id\n",
    "corridor_ids = gdf[\"corridor_group_id_90\"].unique()\n",
    "color_scale = linear.Set1_09.scale(min(corridor_ids), max(corridor_ids))\n",
    "\n",
    "# Add segments to the map\n",
    "for _, row in gdf.iterrows():\n",
    "    color = color_scale(row[\"corridor_group_id_90\"])\n",
    "    folium.GeoJson(\n",
    "        row[\"geometry\"],\n",
    "        style_function=lambda feature, color=color: {\n",
    "            \"color\": color,\n",
    "            \"weight\": 2,\n",
    "            \"fillOpacity\": 0.6,\n",
    "        },\n",
    "        tooltip=f\"Corridor Group: {row['corridor_group_id_90']}<br>Name: {row['stop_pair_name']}<br>Delay: {row['total_ridership_minutes_lost']}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add the color legend\n",
    "color_scale.caption = \"Corridor Group ID\"\n",
    "color_scale.add_to(m)\n",
    "\n",
    "# Save map to file or display\n",
    "m.save(\"la_metro_corridors.html\")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both to WGS 84\n",
    "# processed = processed.to_crs(\"EPSG:4326\")\n",
    "# processed_one = processed_one.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Now concatenate\n",
    "# recombined = pd.concat([processed, processed_one], ignore_index=True)\n",
    "\n",
    "# Export to GeoJSON\n",
    "combined_results.to_file(\"combined_routes.geojson\", driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
