{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pref = \"/Users/joeyshoyer/Downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON files\n",
    "variance_gdf = gpd.read_file(path_pref + \"182_Midday_variance.geojson\")\n",
    "midday_gdf = gpd.read_file(path_pref + \"182_Midday_speeds.geojson\")\n",
    "pm_peak_gdf = gpd.read_file(path_pref + \"182_PM_Peak_speeds.geojson\")\n",
    "am_peak_gdf = gpd.read_file(path_pref + \"182_AM_Peak_speeds.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or None values in stop_id or route_id for each GeoDataFrame\n",
    "variance_gdf = variance_gdf.dropna(subset=['stop_id', 'route_id'])\n",
    "midday_gdf = midday_gdf.dropna(subset=['stop_id', 'route_id'])\n",
    "pm_peak_gdf = pm_peak_gdf.dropna(subset=['stop_id', 'route_id'])\n",
    "am_peak_gdf = am_peak_gdf.dropna(subset=['stop_id', 'route_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column in each GeoDataFrame\n",
    "variance_gdf['stop_route_id'] = variance_gdf['stop_id'].astype(str) + '_' + variance_gdf['route_id'].astype(str)\n",
    "midday_gdf['stop_route_id'] = midday_gdf['stop_id'].astype(str) + '_' + midday_gdf['route_id'].astype(str)\n",
    "pm_peak_gdf['stop_route_id'] = pm_peak_gdf['stop_id'].astype(str) + '_' + pm_peak_gdf['route_id'].astype(str)\n",
    "am_peak_gdf['stop_route_id'] = am_peak_gdf['stop_id'].astype(str) + '_' + am_peak_gdf['route_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_gdf = variance_gdf.drop(columns=[\"geometry\", 'id', 'shape_id', 'stop_sequence', 'fast_slow_ratio', 'trips_per_hour', 'miles_from_last', 'route_short_name', 'route_id', 'stop_name', 'stop_id'])\n",
    "midday_gdf = midday_gdf.drop(columns=['id', 'shape_id', 'stop_sequence', 'direction_id', 'fast_slow_ratio', 'trips_per_hour', 'time_formatted', 'organization_name', 'p20_mph', 'p80_mph', 'stop_id'])\n",
    "pm_peak_gdf = pm_peak_gdf.drop(columns=[\"geometry\", 'id', 'shape_id', 'stop_sequence', 'direction_id', 'fast_slow_ratio', 'trips_per_hour', 'miles_from_last', 'time_formatted', 'organization_name', 'route_short_name', 'route_id', 'stop_name', 'p20_mph', 'p80_mph', 'stop_id'])\n",
    "am_peak_gdf = am_peak_gdf.drop(columns=[\"geometry\", 'id', 'shape_id', 'stop_sequence', 'direction_id', 'fast_slow_ratio', 'trips_per_hour', 'miles_from_last', 'time_formatted', 'organization_name', 'route_short_name', 'route_id', 'stop_name', 'p20_mph', 'p80_mph', 'stop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in pm_peak_gdf\n",
    "midday_gdf.drop_duplicates(subset=['stop_route_id'], keep='first', inplace=True)\n",
    "pm_peak_gdf.drop_duplicates(subset=['stop_route_id'], keep='first', inplace=True)\n",
    "am_peak_gdf.drop_duplicates(subset=['stop_route_id'], keep='first', inplace=True)\n",
    "variance_gdf.drop_duplicates(subset=['stop_route_id'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midday_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_peak_gdf.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_peak_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on stop_name\n",
    "merged_gdf = midday_gdf.merge(pm_peak_gdf, on=\"stop_route_id\", how=\"inner\", suffixes=('_midday', '_pm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.merge(am_peak_gdf, on=\"stop_route_id\", how=\"inner\", suffixes=('', '_am'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_gdf = merged_gdf.merge(variance_gdf, on=\"stop_route_id\", how=\"inner\", suffixes=('', '_var'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.rename(columns={'p50_mph': 'p50_mph_am'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of each p50_mph column\n",
    "mean_midday = merged_gdf['p50_mph_midday'].mean()\n",
    "mean_pm = merged_gdf['p50_mph_pm'].mean()\n",
    "mean_am = merged_gdf['p50_mph_am'].mean()\n",
    "\n",
    "\n",
    "print(mean_midday, mean_pm, mean_am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference from the average for each p50_mph column\n",
    "merged_gdf['diff_from_avg_midday'] = mean_midday - merged_gdf['p50_mph_midday']\n",
    "merged_gdf['diff_from_avg_pm'] = mean_pm - merged_gdf['p50_mph_pm']\n",
    "merged_gdf['diff_from_avg_am'] = mean_am - merged_gdf['p50_mph_am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json(path_pref + \"ridership.json\")\n",
    "json_df['line_name'] = json_df['line_name'].astype(str)\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort by line_name, year, and month in descending order\n",
    "json_df_sorted = json_df.sort_values(by=['line_name', 'year', 'month'], ascending=[True, False, False])\n",
    "\n",
    "# Step 2: Drop duplicates based on line_name, keeping the first (most recent) record\n",
    "json_df_most_recent = json_df_sorted.drop_duplicates(subset='line_name', keep='first')\n",
    "\n",
    "# Step 3: Inspect the result\n",
    "print(json_df_most_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_most_recent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_most_recent.sort_values(by='est_wkday_ridership', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.merge(json_df_most_recent, left_on='route_short_name', right_on='line_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time lost for midday, pm, and am\n",
    "merged_gdf['time_lost_midday'] = (merged_gdf['miles_from_last'] / merged_gdf['diff_from_avg_midday']) * 60\n",
    "merged_gdf['time_lost_pm'] = (merged_gdf['miles_from_last'] / merged_gdf['diff_from_avg_pm']) * 60\n",
    "merged_gdf['time_lost_am'] = (merged_gdf['miles_from_last'] / merged_gdf['diff_from_avg_am']) * 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_gdf['ridership_minutes_lost_midday'] = merged_gdf['time_lost_midday'] * merged_gdf['est_wkday_ridership']\n",
    "merged_gdf['ridership_minutes_lost_pm'] = merged_gdf['time_lost_pm'] * merged_gdf['est_wkday_ridership']\n",
    "merged_gdf['ridership_minutes_lost_am'] = merged_gdf['time_lost_am'] * merged_gdf['est_wkday_ridership']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf[['stop_name', 'route_short_name', 'ridership_minutes_lost_midday', 'ridership_minutes_lost_pm', 'ridership_minutes_lost_am']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate (passenger-hour wasted) / (kilometer travelled)\n",
    "merged_gdf['passenger_hour_per_mi_midday'] = (merged_gdf['ridership_minutes_lost_midday'] / 60) / merged_gdf['miles_from_last']\n",
    "merged_gdf['passenger_hour_per_mi_pm'] = (merged_gdf['ridership_minutes_lost_pm'] / 60) / merged_gdf['miles_from_last']\n",
    "merged_gdf['passenger_hour_per_mi_am'] = (merged_gdf['ridership_minutes_lost_am'] / 60) / merged_gdf['miles_from_last']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"Top 10 segments with highest (passenger-hour wasted) / (mile travelled) for AM peak:\")\n",
    "print(merged_gdf.sort_values('passenger_hour_per_mi_am', ascending=False)[['stop_route_id', 'route_short_name', 'stop_name', 'passenger_hour_per_mi_am']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 segments with highest (passenger-hour wasted) / (mile travelled) for PM peak:\")\n",
    "print(merged_gdf.sort_values('passenger_hour_per_mi_pm', ascending=False)[['stop_route_id', 'route_short_name', 'stop_name', 'passenger_hour_per_mi_pm']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 segments with highest (passenger-hour wasted) / (mile travelled) for Midday:\")\n",
    "print(merged_gdf.sort_values('passenger_hour_per_mi_midday', ascending=False)[['stop_route_id', 'route_short_name', 'stop_name', 'passenger_hour_per_mi_midday']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by route\n",
    "route_aggregated = merged_gdf.groupby('route_short_name').agg({\n",
    "    'passenger_hour_per_mi_am': 'mean',\n",
    "    'passenger_hour_per_mi_pm': 'mean',\n",
    "    'passenger_hour_per_mi_midday': 'mean',\n",
    "    'est_wkday_ridership': 'first'  # Assuming ridership is the same for all segments of a route\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 routes with highest average (passenger-hour wasted) / (mile travelled) for AM peak:\")\n",
    "print(route_aggregated.sort_values('passenger_hour_per_mi_am', ascending=False)[['route_short_name', 'passenger_hour_per_mi_am', 'est_wkday_ridership']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 routes with highest average (passenger-hour wasted) / (mile travelled) for PM peak:\")\n",
    "print(route_aggregated.sort_values('passenger_hour_per_mi_pm', ascending=False)[['route_short_name', 'passenger_hour_per_mi_pm', 'est_wkday_ridership']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTop 10 routes with highest average (passenger-hour wasted) / (mile travelled) for Midday:\")\n",
    "print(route_aggregated.sort_values('passenger_hour_per_mi_midday', ascending=False)[['route_short_name', 'passenger_hour_per_mi_midday', 'est_wkday_ridership']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total ridership hours wasted for each time period\n",
    "merged_gdf['ridership_hours_lost_midday'] = merged_gdf['ridership_minutes_lost_midday'] / 60\n",
    "merged_gdf['ridership_hours_lost_pm'] = merged_gdf['ridership_minutes_lost_pm'] / 60\n",
    "merged_gdf['ridership_hours_lost_am'] = merged_gdf['ridership_minutes_lost_am'] / 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by route\n",
    "route_aggregated = merged_gdf.groupby('route_short_name').agg({\n",
    "    'ridership_hours_lost_midday': 'sum',\n",
    "    'ridership_hours_lost_pm': 'sum',\n",
    "    'ridership_hours_lost_am': 'sum',\n",
    "    'est_wkday_ridership': 'first'  # Assuming ridership is the same for all segments of a route\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total ridership hours wasted across all time periods\n",
    "merged_gdf['total_ridership_hours_lost'] = (\n",
    "    merged_gdf['ridership_hours_lost_midday'] +\n",
    "    merged_gdf['ridership_hours_lost_pm'] +\n",
    "    merged_gdf['ridership_hours_lost_am']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by route\n",
    "route_aggregated = merged_gdf.groupby('route_short_name').agg({\n",
    "    'total_ridership_hours_lost': 'sum',\n",
    "    'est_wkday_ridership': 'first'  # Assuming ridership is the same for all segments of a route\n",
    "}).reset_index()\n",
    "\n",
    "# Sort the results by total ridership hours lost\n",
    "route_aggregated_sorted = route_aggregated.sort_values('total_ridership_hours_lost', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 20 routes with highest total ridership hours wasted\n",
    "print(\"Top 20 routes with highest total ridership hours wasted:\")\n",
    "print(route_aggregated_sorted[['route_short_name', 'total_ridership_hours_lost', 'est_wkday_ridership']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert GeoDataFrame to a GeoJSON-like Python dictionary\n",
    "geojson_dict = merged_gdf.__geo_interface__\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"bus_segments.geojson\", \"w\") as f:\n",
    "    json.dump(geojson_dict, f)\n",
    "\n",
    "print(\"Data saved successfully to bus_segments.geojson\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
