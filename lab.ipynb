{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import pyogrio\n",
    "import fiona\n",
    "from shapely.ops import unary_union\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "import shapely.speedups\n",
    "if shapely.speedups.available:\n",
    "    shapely.speedups.enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pref = \"/Users/joeyshoyer/Downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridership_data = gpd.read_file(path_pref + \"Average_weekday_ridership.geojson\")\n",
    "ridership_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON files\n",
    "midday_gdf = gpd.read_file(path_pref + \"182_Midday_speeds.geojson\")\n",
    "pm_peak_gdf = gpd.read_file(path_pref + \"182_PM_Peak_speeds.geojson\")\n",
    "am_peak_gdf = gpd.read_file(path_pref + \"182_AM_Peak_speeds.geojson\")\n",
    "am_peak_gdf = gpd.read_file(path_pref + \"182_AM_Peak_speeds.geojson\")\n",
    "\n",
    "print(len(midday_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments_data = gpd.read_file(\"segments.geojson\")\n",
    "# segments_data = segments_data.drop(columns=['segment_id', 'row_id', 'trip_id', 'traversals', 'traversal_time', 'speed', 'stop_id2'])\n",
    "# segments_data = segments_data.rename(columns={\"stop_id1\": \"stop_id\"})\n",
    "\n",
    "# def split_before_dash(df, source_column, new_column):\n",
    "#     df[new_column] = pd.to_numeric(df[source_column].str.split('-').str[0], errors='coerce')\n",
    "#     return df\n",
    "\n",
    "# segments_data = split_before_dash(segments_data, 'route_id', 'route_short_name_numeric')\n",
    "# segments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "midday_gdf = midday_gdf.drop(columns=['id', 'fast_slow_ratio', 'trips_per_hour', 'time_formatted', 'organization_name'])\n",
    "pm_peak_gdf = pm_peak_gdf.drop(columns=[\"geometry\", 'id', 'direction_id', 'fast_slow_ratio', 'trips_per_hour', 'miles_from_last', 'time_formatted', 'organization_name', 'route_short_name', 'route_id', 'stop_name', 'p20_mph', 'p80_mph', 'stop_id'])\n",
    "am_peak_gdf = am_peak_gdf.drop(columns=[\"geometry\", 'id', 'direction_id', 'fast_slow_ratio', 'trips_per_hour', 'miles_from_last', 'time_formatted', 'organization_name', 'route_short_name', 'route_id', 'stop_name', 'p20_mph', 'p80_mph', 'stop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "midday_gdf['uid'] = midday_gdf['shape_id'] + midday_gdf['stop_sequence'].astype(str)\n",
    "pm_peak_gdf['uid'] = pm_peak_gdf['shape_id'] + pm_peak_gdf['stop_sequence'].astype(str)\n",
    "am_peak_gdf['uid'] = am_peak_gdf['shape_id'] + am_peak_gdf['stop_sequence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on stop_name\n",
    "merged_gdf = midday_gdf.merge(pm_peak_gdf, on=\"uid\", how=\"inner\", suffixes=('_midday', '_pm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.merge(am_peak_gdf, on=\"uid\", how=\"inner\", suffixes=('', '_am'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.rename(columns={'p50_mph': 'p50_mph_am'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.drop(columns=['shape_id_midday', 'stop_sequence_midday', 'shape_id', 'stop_sequence_pm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf[~merged_gdf['route_id'].str.startswith('8')]\n",
    "merged_gdf = merged_gdf[~merged_gdf['route_id'].str.startswith('9')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for midday\n",
    "# midday_filtered = merged_gdf[merged_gdf['diff_from_avg_midday'] > 0].copy()\n",
    "\n",
    "# # Filter for AM peak\n",
    "# am_filtered = merged_gdf[merged_gdf['diff_from_avg_am'] > 0].copy()\n",
    "\n",
    "# # Filter for PM peak\n",
    "# pm_filtered = merged_gdf[merged_gdf['diff_from_avg_pm'] > 0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_route_name(value):\n",
    "    if pd.isna(value):\n",
    "        print(\"Encountered a null value in route_short_name.\")\n",
    "        return value\n",
    "    \n",
    "    value = str(value)  # Convert to string to handle potential float values\n",
    "    \n",
    "    if '/' in value:\n",
    "        value = value.split('/')[0]\n",
    "    \n",
    "    value = value.strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "    if value == '':\n",
    "        print(\"Encountered an empty string in route_short_name after processing.\")\n",
    "        return value\n",
    "    \n",
    "    try:\n",
    "        int(value)\n",
    "        #return value\n",
    "    except ValueError:\n",
    "        print(f\"Value '{value}' in route_short_name is not convertible to an integer.\")\n",
    "        #return value\n",
    "    return value\n",
    "\n",
    "# Apply the function to the 'route_short_name' column\n",
    "merged_gdf['route_short_name'] = merged_gdf['route_short_name'].apply(process_route_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure route_short_name is treated as a string for initial checks\n",
    "merged_gdf['route_short_name'] = merged_gdf['route_short_name'].astype(str)\n",
    "\n",
    "# Extract numeric part of route_short_name if needed\n",
    "# For this example, let's assume route_short_name has a numeric part that we want to convert\n",
    "# If route_short_name is already numeric, you can skip this step\n",
    "merged_gdf['route_short_name_numeric'] = merged_gdf['route_short_name'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Convert to integer, handling any potential issues\n",
    "merged_gdf['route_short_name_numeric'] = merged_gdf['route_short_name_numeric'].fillna(0).astype(int)\n",
    "\n",
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column in each GeoDataFrame\n",
    "merged_gdf['stop_route_dir_id'] = merged_gdf['stop_id'].astype(str) + '_' + merged_gdf['route_id'].astype(str) + '_' + merged_gdf['direction_id'].astype(str)\n",
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf['speed_diff'] = merged_gdf['p80_mph'] - merged_gdf['p50_mph_midday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates, keep the one with the highest difference in speed\n",
    "\n",
    "# Separate rows with missing 'stop_id'\n",
    "missing_stop_id = merged_gdf[merged_gdf['stop_id'].isna()]\n",
    "print(len(missing_stop_id))\n",
    "\n",
    "\n",
    "# Filter out rows with non-missing 'stop_id'\n",
    "non_missing_gdf = merged_gdf[merged_gdf['stop_id'].notnull()]\n",
    "print(len(non_missing_gdf))\n",
    "\n",
    "# Group by 'stop_route_dir_id' and get the index of the max 'speed_diff'\n",
    "max_speed_diff_indices = non_missing_gdf.groupby('stop_route_dir_id')['speed_diff'].idxmax()\n",
    "\n",
    "# Create a new GeoDataFrame with the rows corresponding to the max indices\n",
    "cleaned_non_missing_gdf = non_missing_gdf.loc[max_speed_diff_indices].reset_index(drop=True)\n",
    "\n",
    "# Combine cleaned non-missing rows with rows that had missing 'stop_route_dir_id'\n",
    "cleaned_gdf = pd.concat([cleaned_non_missing_gdf, missing_stop_id], ignore_index=True)\n",
    "\n",
    "# Print the cleaned GeoDataFrame\n",
    "print(\"Cleaned GeoDataFrame (duplicates removed, missing stop IDs preserved):\")\n",
    "cleaned_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = cleaned_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates in the 'stop_route_dir_id' column\n",
    "duplicates = cleaned_gdf[cleaned_gdf.duplicated(subset=['stop_route_dir_id'], keep=False)]\n",
    "\n",
    "# Print the duplicates\n",
    "print(\"Duplicates based on 'stop_route_dir_id':\")\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "ridership_full = pd.read_excel(path_pref + 'Bus Ridership CPRA 24-1647.xlsx', sheet_name='2024-03')\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(ridership_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridership_full = ridership_full[ridership_full['DAY TYPE'] == 'DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridership_full['DIRECTION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridership_full['YEAR MONTH'] = ridership_full['YEAR MONTH'].astype(str)\n",
    "ridership_full['LINE'] = ridership_full['LINE'].astype(str)\n",
    "ridership_full['DIRECTION_bin'] = ridership_full['DIRECTION'].map({'North': 0, 'South': 1, 'East': 0, 'West': 1})\n",
    "ridership_full['STOP ID'] = ridership_full['STOP ID'].astype(str)\n",
    "\n",
    "# Create a unique identifier for joining in ridership_full\n",
    "ridership_full['stop_route_short_dir_id'] = (\n",
    "    ridership_full['STOP ID'] + '_' + \n",
    "    ridership_full['LINE'] + '_' + \n",
    "    ridership_full['DIRECTION_bin'].astype(str)\n",
    ")\n",
    "\n",
    "# Create the same identifier in merged_gdf\n",
    "merged_gdf['stop_route_short_dir_id'] = (\n",
    "    merged_gdf['stop_id'].astype(str) + '_' + \n",
    "    merged_gdf['route_short_name'].astype(str) + '_' + \n",
    "    merged_gdf['direction_id'].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridership_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate ridership data to handle duplicates\n",
    "ridership_agg = ridership_full.groupby('stop_route_short_dir_id').agg({\n",
    "    'Avg Ons': 'mean',\n",
    "    'Avg Offs': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create a dictionary for efficient mapping\n",
    "ridership_dict = ridership_agg.set_index('stop_route_short_dir_id')[['Avg Ons', 'Avg Offs']].to_dict(orient='index')\n",
    "\n",
    "# Use map to add Avg Ons and Avg Offs to merged_gdf\n",
    "merged_gdf['Avg Ons'] = merged_gdf['stop_route_short_dir_id'].map(lambda x: ridership_dict.get(x, {}).get('Avg Ons'))\n",
    "merged_gdf['Avg Offs'] = merged_gdf['stop_route_short_dir_id'].map(lambda x: ridership_dict.get(x, {}).get('Avg Offs'))\n",
    "\n",
    "\n",
    "# Check if the number of rows remained the same\n",
    "print(f\"Original merged_gdf rows: {len(merged_gdf)}\")\n",
    "print(f\"Final merged data rows: {len(merged_gdf)}\")\n",
    "\n",
    "# Check for any null values in the new columns\n",
    "print(f\"Null values in Avg Ons: {merged_gdf['Avg Ons'].isnull().sum()}\")\n",
    "print(f\"Null values in Avg Offs: {merged_gdf['Avg Offs'].isnull().sum()}\")\n",
    "\n",
    "# Check for unique values in stop_route_short_dir_id\n",
    "print(f\"Unique stop_route_short_dir_id in merged_gdf: {merged_gdf['stop_route_short_dir_id'].nunique()}\")\n",
    "print(f\"Unique stop_route_short_dir_id in ridership_full: {ridership_full['stop_route_short_dir_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_route_ridership(data, route, direction):\n",
    "    # Filter the dataframe for the specific route and direction\n",
    "    route_df = data[\n",
    "        (data['route_short_name'] == str(route)) & \n",
    "        (data['direction_id'] == direction)\n",
    "    ].sort_values('stop_sequence')\n",
    "\n",
    "    # Initialize variables\n",
    "    riders_on_bus = 0\n",
    "    segment_ridership = {}\n",
    "\n",
    "    print(f\"\\nCalculating ridership for route {route}, direction {direction}\")\n",
    "\n",
    "    for _, row in route_df.iterrows():\n",
    "        print(row['stop_sequence'])\n",
    "        stop_id = int(row['stop_id'] or '0')\n",
    "        stop_name = row['stop_name']\n",
    "        stop_route_short_dir_id = row['stop_route_short_dir_id']\n",
    "        \n",
    "        riders_getting_on = row['Avg Ons'] if pd.notna(row['Avg Ons']) else 0\n",
    "        riders_getting_off = row['Avg Offs'] if pd.notna(row['Avg Offs']) else 0\n",
    "        \n",
    "        # Update riders on bus\n",
    "        riders_on_bus += riders_getting_on - riders_getting_off\n",
    "        riders_on_bus = max(0, riders_on_bus)  # Ensure it doesn't go below zero\n",
    "\n",
    "        segment_ridership[stop_route_short_dir_id] = riders_on_bus\n",
    "\n",
    "        print(f\"Stop {stop_id} - {stop_name}: On: {riders_getting_on:.2f}, Off: {riders_getting_off:.2f}, On Bus: {riders_on_bus:.2f}\")\n",
    "\n",
    "    return segment_ridership\n",
    "\n",
    "# Test the function with route \"4\" in direction 0\n",
    "test_route = \"720\"\n",
    "test_direction = 0\n",
    "\n",
    "# Assuming 'data' is your DataFrame with all the necessary columns\n",
    "result = calculate_route_ridership(merged_gdf, test_route, test_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_routes_ridership(merged_data):\n",
    "    all_segment_ridership = {}\n",
    "    # Get unique combinations of route_short_name and direction_id\n",
    "    route_directions = merged_data[['route_short_name', 'direction_id']].drop_duplicates()\n",
    "    \n",
    "    for _, row in route_directions.iterrows():\n",
    "        route = row['route_short_name']\n",
    "        direction = row['direction_id']\n",
    "        print(f\"Calculating ridership for route {route}, direction {direction}\")\n",
    "        segment_ridership = calculate_route_ridership(merged_data, route, direction)\n",
    "        all_segment_ridership.update(segment_ridership)\n",
    "    \n",
    "    return all_segment_ridership\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ridership for all routes\n",
    "all_routes_ridership = calculate_all_routes_ridership(merged_gdf)\n",
    "\n",
    "# Add the calculated ridership to the original DataFrame\n",
    "merged_gdf['calculated_segment_ridership'] = merged_gdf['stop_route_short_dir_id'].map(all_routes_ridership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Assuming your GeoDataFrame is named 'gdf'\n",
    "# If it's not, replace 'gdf' with the actual variable name\n",
    "\n",
    "# Handle null values in simulated_segment_ridership\n",
    "#merged_gdf['simulated_segment_ridership'] = merged_gdf['simulated_segment_ridership'].fillna(0)\n",
    "\n",
    "\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = merged_gdf['calculated_segment_ridership'].min()\n",
    "vmax = merged_gdf['calculated_segment_ridership'].max()\n",
    "color_map = LinearColormap(colors=['blue', 'yellow', 'red'], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature['properties']['calculated_segment_ridership']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = 'gray'  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\n",
    "        'color': color,\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7\n",
    "    }\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature['properties']\n",
    "    ridership = props['calculated_segment_ridership']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_name']}<br>\"\n",
    "        f\"Route: {props['route_short_name']}<br>\"\n",
    "        f\"Ridership: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "# Add the GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    merged_gdf,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['stop_name', 'route_short_name', 'calculated_segment_ridership'],\n",
    "                                  aliases=['Stop Name:', 'Route:', 'Ridership:'],\n",
    "                                  localize=True,\n",
    "                                  sticky=False,\n",
    "                                  labels=True,\n",
    "                                  style=\"\"\"\n",
    "                                      background-color: #F0EFEF;\n",
    "                                      border: 2px solid black;\n",
    "                                      border-radius: 3px;\n",
    "                                      box-shadow: 3px;\n",
    "                                  \"\"\",\n",
    "                                  max_width=800,)\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = '''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "             '''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "m.save('los_angeles_bus_ridership_map.html')\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoDataFrame to a GeoJSON-like Python dictionary\n",
    "geojson_dict = merged_gdf.__geo_interface__\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"bus_segments_calculated_ridership.geojson\", \"w\") as f:\n",
    "    json.dump(geojson_dict, f)\n",
    "\n",
    "print(\"Data saved successfully to bus_segments.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def compare_speed_calculations(gdf):\n",
    "    \"\"\"\n",
    "    Compare two methods of speed calculation and create visualizations\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame for comparison\n",
    "    comparison_df = pd.DataFrame()\n",
    "    \n",
    "    # Calculate both methods\n",
    "  # Calculate both methods correctly\n",
    "    comparison_df['min_method'] = gdf[['p50_mph_midday', 'p50_mph_pm', 'p50_mph_am']].min(axis=1)\n",
    "    comparison_df['avg_method'] = (gdf['p50_mph_midday'] + gdf['p50_mph_pm'] + gdf['p50_mph_am']) / 3\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'Minimum Method': comparison_df['min_method'].describe(),\n",
    "        'Average Method': comparison_df['avg_method'].describe()\n",
    "    })\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    additional_stats = pd.DataFrame({\n",
    "        'Metric': ['Skewness', 'Kurtosis', 'Median'],\n",
    "        'Minimum Method': [\n",
    "            comparison_df['min_method'].skew(),\n",
    "            comparison_df['min_method'].kurtosis(),\n",
    "            comparison_df['min_method'].median()\n",
    "        ],\n",
    "        'Average Method': [\n",
    "            comparison_df['avg_method'].skew(),\n",
    "            comparison_df['avg_method'].kurtosis(),\n",
    "            comparison_df['avg_method'].median()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Histogram\n",
    "    sns.histplot(data=comparison_df, x='min_method', ax=axes[0,0], label='Minimum Method', alpha=0.5)\n",
    "    sns.histplot(data=comparison_df, x='avg_method', ax=axes[0,0], label='Average Method', alpha=0.5)\n",
    "    axes[0,0].set_title('Distribution of Speed Calculations')\n",
    "    axes[0,0].set_xlabel('Speed (mph)')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    comparison_df_melted = pd.melt(comparison_df)\n",
    "    sns.boxplot(data=comparison_df_melted, x='variable', y='value', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Box Plot of Speed Calculations')\n",
    "    axes[0,1].set_xlabel('Method')\n",
    "    axes[0,1].set_ylabel('Speed (mph)')\n",
    "    \n",
    "    # Q-Q plot for minimum method\n",
    "    stats.probplot(comparison_df['min_method'], dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot - Minimum Method')\n",
    "    \n",
    "    # Q-Q plot for average method\n",
    "    stats.probplot(comparison_df['avg_method'], dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot - Average Method')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate the difference between methods\n",
    "    comparison_df['difference'] = comparison_df['min_method'] - comparison_df['avg_method']\n",
    "    difference_stats = comparison_df['difference'].describe()\n",
    "    \n",
    "    return summary_stats, additional_stats, difference_stats, comparison_df, fig\n",
    "\n",
    "# Run the comparison\n",
    "summary_stats, additional_stats, difference_stats, comparison_df, fig = compare_speed_calculations(merged_gdf)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(summary_stats)\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(additional_stats)\n",
    "print(\"\\nDifference Statistics (Minimum - Average):\")\n",
    "print(difference_stats)\n",
    "\n",
    "# Display the plot (if in a notebook)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delay_effects(gdf):\n",
    "    \"\"\"\n",
    "    Calculate delay effects for the consolidated segments using average p50 speed across time periods.\n",
    "    \"\"\"\n",
    "    # Calculate average p50 speed across the three time periods\n",
    "    #gdf['p50_mph_avg'] = gdf[['p50_mph_midday', 'p50_mph_pm', 'p50_mph_am']].min(axis=1)\n",
    "    gdf['p50_mph_avg'] = (gdf['p50_mph_midday'] + gdf['p50_mph_pm'] + gdf['p50_mph_am'] )/ 3\n",
    "    #gdf['p50_mph_avg'] = ((gdf['p50_mph_midday'] * 20) + (gdf['p50_mph_pm'] * 40) + (gdf['p50_mph_am'] * 40) ) / 100\n",
    "\n",
    "    # Calculate difference from average speed\n",
    "    gdf['diff_from_avg'] = gdf['p80_mph'] - gdf['p50_mph_avg']\n",
    "    \n",
    "    gdf['actual_time'] = gdf['miles_from_last'] / gdf['p50_mph_avg']\n",
    "    gdf['best_time'] = gdf['miles_from_last'] / gdf['p80_mph']\n",
    "\n",
    "\n",
    "    gdf['delay_hours'] = np.maximum(0, gdf['actual_time'] - gdf['best_time'])\n",
    "    gdf['time_lost'] = gdf['delay_hours'] * 60\n",
    "\n",
    "    \n",
    "    # Calculate total ridership minutes lost\n",
    "    gdf['total_ridership_minutes_lost'] = gdf['time_lost'] * gdf['calculated_segment_ridership']\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = calculate_delay_effects(merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_to_merge = merged_gdf[['stop_id', 'stop_name', 'route_id', 'route_short_name', 'direction_id',\n",
    "        'miles_from_last', 'geometry', 'uid',\n",
    "       'stop_sequence',\n",
    "       'stop_route_short_dir_id',\n",
    "       'calculated_segment_ridership', 'p50_mph_avg', 'p80_mph', 'diff_from_avg',\n",
    "       'time_lost', 'total_ridership_minutes_lost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments_data = gpd.read_file(\"segments.geojson\")\n",
    "# segments_data = segments_data.drop(columns=['segment_id', 'row_id', 'trip_id', 'traversals', 'traversal_time', 'speed', 'stop_id2'])\n",
    "# segments_data = segments_data.rename(columns={\"stop_id1\": \"stop_id\"})\n",
    "\n",
    "# def split_before_dash(df, source_column, new_column):\n",
    "#     df[new_column] = pd.to_numeric(df[source_column].str.split('-').str[0], errors='coerce')\n",
    "#     return df\n",
    "\n",
    "# segments_data = split_before_dash(segments_data, 'route_id', 'route_short_name_numeric')\n",
    "# segments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined = merged_consolidated_gdf.merge(segments_data, how='left', on=['route_short_name_numeric', 'stop_id', 'direction_id'])\n",
    "# combined = merged_consolidated_gdf\n",
    "# combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_without_expansion(merged_consolidated_gdf, segments_data):\n",
    "    # Ensure we're working with copies to avoid modifying the original dataframes\n",
    "    merged_gdf = merged_consolidated_gdf.copy()\n",
    "    segments = segments_data.copy()\n",
    "\n",
    "    # Remove rows where route_short_name is None or NaN\n",
    "    merged_gdf = merged_gdf.dropna(subset=['route_short_name'])\n",
    "    \n",
    "    # Verify the removal\n",
    "    print(f\"Rows in merged_gdf after removing None/NaN route_short_name: {len(merged_gdf)}\")\n",
    "\n",
    "    # Check for duplicates in the merge keys\n",
    "    merge_keys = ['route_short_name_numeric', 'stop_id', 'direction_id']\n",
    "    \n",
    "    print(\"\\nChecking for duplicates in merged_gdf:\")\n",
    "    print(merged_gdf[merged_gdf.duplicated(subset=merge_keys, keep=False)])\n",
    "    \n",
    "    print(\"\\nChecking for duplicates in segments:\")\n",
    "    print(segments[segments.duplicated(subset=merge_keys, keep=False)])\n",
    "\n",
    "    # Handle duplicates if necessary\n",
    "    # For example, keep the first occurrence of each duplicate\n",
    "    merged_gdf = merged_gdf.drop_duplicates(subset=merge_keys, keep='first')\n",
    "    segments = segments.drop_duplicates(subset=merge_keys, keep='first')\n",
    "\n",
    "    # Perform the merge\n",
    "    combined = merged_gdf.merge(\n",
    "        segments,\n",
    "        how='left',\n",
    "        on=merge_keys,\n",
    "        indicator=True  # This will help us see which rows were matched\n",
    "    )\n",
    "\n",
    "    # Check the merge results\n",
    "    print(\"\\nMerge results:\")\n",
    "    print(combined['_merge'].value_counts())\n",
    "\n",
    "    # Remove the indicator column\n",
    "    combined = combined.drop(columns=['_merge'])\n",
    "\n",
    "    # Verify the final row count\n",
    "    print(f\"\\nFinal row count: {len(combined)}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "# try:\n",
    "#     combined = merge_without_expansion(merged_consolidated_gdf, segments_data)\n",
    "# except pd.errors.MergeError as e:\n",
    "#     print(f\"MergeError occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_to_merge['direction_id'] = gdf_to_merge['direction_id'].astype(str)\n",
    "gdf_to_merge['stop_sequence'] = gdf_to_merge['stop_sequence'].astype(str)\n",
    "gdf_to_merge['p50_mph_avg'] = gdf_to_merge['p50_mph_avg'].astype(str)\n",
    "gdf_to_merge['p80_mph'] = gdf_to_merge['p80_mph'].astype(str)\n",
    "gdf_to_merge['diff_from_avg'] = gdf_to_merge['diff_from_avg'].astype(str)\n",
    "gdf_to_merge['calculated_segment_ridership_str'] = gdf_to_merge['calculated_segment_ridership'].astype(str)\n",
    "gdf_to_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_to_merge_dir_zero = gdf_to_merge[gdf_to_merge['direction_id'] == '0.0']\n",
    "gdf_to_merge_dir_one = gdf_to_merge[gdf_to_merge['direction_id'] == '1.0']\n",
    "print(gdf_to_merge_dir_zero.info())\n",
    "print(gdf_to_merge_dir_one.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRS84 is equivalent to EPSG:4326 with longitude/latitude ordering\n",
    "whole_crs = gdf_to_merge.set_crs('EPSG:4326', allow_override=True)\n",
    "gdf_to_merge_dir_zero_crs = gdf_to_merge_dir_zero.set_crs('EPSG:4326', allow_override=True)\n",
    "gdf_to_merge_dir_one_crs = gdf_to_merge_dir_one.set_crs('EPSG:4326', allow_override=True)\n",
    "\n",
    "# Check lengths in decimal degrees\n",
    "print(\"Lengths in decimal degrees:\")\n",
    "print(gdf_to_merge_dir_zero_crs.geometry.length.describe())\n",
    "\n",
    "# Convert to UTM Zone 11N (appropriate for LA area) and check lengths\n",
    "gdf_to_merge_dir_zero_crs = gdf_to_merge_dir_zero_crs.to_crs('EPSG:32611')\n",
    "print(\"\\nLengths in meters:\")\n",
    "print(gdf_to_merge_dir_zero_crs.geometry.length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geom_ov_fast(gdf, overlap_threshold=0.8, debug=True):\n",
    "    \"\"\"\n",
    "    Optimized version of merge_geom_ov using spatial indexing and sparse matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame with polygon geometries\n",
    "    overlap_threshold (float): Minimum overlap ratio required for merging (0.0 to 1.0)\n",
    "    debug (bool): Whether to print debug messages\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: Processed GeoDataFrame with selectively merged segments\n",
    "    \"\"\"\n",
    "    def log_debug(message):\n",
    "        if debug:\n",
    "            print(message)\n",
    "\n",
    "    try:\n",
    "        log_debug(\"Starting optimized segment merging...\")\n",
    "        \n",
    "        # Create spatial index\n",
    "        spatial_index = gdf.sindex\n",
    "        \n",
    "        # Pre-compute areas for all polygons\n",
    "        areas = np.array([geom.area for geom in gdf.geometry])\n",
    "        \n",
    "        # Initialize sparse matrix for connectivity\n",
    "        n = len(gdf)\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        \n",
    "        # Function to check overlap ratio\n",
    "        def check_overlap(idx1, idx2):\n",
    "            if idx1 == idx2:\n",
    "                return False\n",
    "                \n",
    "            geom1 = gdf.geometry.iloc[idx1]\n",
    "            geom2 = gdf.geometry.iloc[idx2]\n",
    "            \n",
    "            if not (isinstance(geom1, Polygon) and isinstance(geom2, Polygon)):\n",
    "                return False\n",
    "                \n",
    "            intersection_area = geom1.intersection(geom2).area\n",
    "            min_area = min(areas[idx1], areas[idx2])\n",
    "            \n",
    "            if min_area == 0:\n",
    "                return False\n",
    "                \n",
    "            return (intersection_area / min_area) >= overlap_threshold\n",
    "\n",
    "        # Use spatial index to find potential overlaps\n",
    "        for idx in range(n):\n",
    "            geom = gdf.geometry.iloc[idx]\n",
    "            bounds = geom.bounds\n",
    "            \n",
    "            # Get potential neighbors using spatial index\n",
    "            potential_matches_idx = list(spatial_index.intersection(bounds))\n",
    "            \n",
    "            # Filter actual overlaps\n",
    "            for match_idx in potential_matches_idx:\n",
    "                if match_idx <= idx:  # Only check each pair once\n",
    "                    continue\n",
    "                    \n",
    "                if check_overlap(idx, match_idx):\n",
    "                    rows.extend([idx, match_idx])\n",
    "                    cols.extend([match_idx, idx])\n",
    "                    data.extend([1, 1])\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        adjacency_matrix = csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "        \n",
    "        # Find connected components\n",
    "        n_components, labels = connected_components(adjacency_matrix, directed=False)\n",
    "        \n",
    "        log_debug(f\"Found {n_components} distinct groups\")\n",
    "        \n",
    "        # Create new GeoDataFrame with merged geometries\n",
    "        new_rows = []\n",
    "        \n",
    "        for component_id in range(n_components):\n",
    "            # Get indices for this component\n",
    "            component_indices = np.where(labels == component_id)[0]\n",
    "            \n",
    "            if len(component_indices) == 1:\n",
    "                # Single segment, no merging needed\n",
    "                new_rows.append(gdf.iloc[component_indices[0]])\n",
    "            else:\n",
    "                # Get the segments to merge\n",
    "                group_data = gdf.iloc[component_indices]\n",
    "                \n",
    "                # Merge geometries using unary_union\n",
    "                merged_geom = group_data.geometry.unary_union\n",
    "                \n",
    "                # Aggregate non-geometric data\n",
    "                agg_data = {}\n",
    "                for col in group_data.columns:\n",
    "                    if col == 'geometry':\n",
    "                        continue\n",
    "                    elif col == 'miles_from_last':\n",
    "                        agg_data[col] = group_data[col].max()\n",
    "                    elif pd.api.types.is_numeric_dtype(group_data[col]):\n",
    "                        agg_data[col] = group_data[col].sum()\n",
    "                    else:\n",
    "                        values = group_data[col].dropna().unique()\n",
    "                        agg_data[col] = ', '.join(str(v) for v in values)\n",
    "                \n",
    "                agg_data['geometry'] = merged_geom\n",
    "                new_rows.append(pd.Series(agg_data))\n",
    "        \n",
    "        # Create final GeoDataFrame\n",
    "        merged_gdf = gpd.GeoDataFrame(new_rows, crs=gdf.crs)\n",
    "        log_debug(f\"Final segment count: {len(merged_gdf)}\")\n",
    "        \n",
    "        return merged_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_debug(f\"Error during merging: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # shorten_distance and buffer_width are now in meters\n",
    "    processed = merge_geom_ov_fast(gdf_to_merge_dir_zero_crs)     \n",
    "    processed_one = merge_geom_ov_fast(gdf_to_merge_dir_one)     \n",
    "\n",
    "    if processed is None:\n",
    "        print(\"Processing failed - check the error messages above\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # shorten_distance and buffer_width are now in meters\n",
    "    whole_crs_proc = merge_geom_ov_fast(whole_crs)     \n",
    "\n",
    "    if processed is None:\n",
    "        print(\"Processing failed - check the error messages above\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['minutes_lost_per_mile'] = processed['total_ridership_minutes_lost'] / processed['miles_from_last']\n",
    "processed_one['minutes_lost_per_mile'] = processed_one['total_ridership_minutes_lost'] / processed_one['miles_from_last']\n",
    "whole_crs_proc['minutes_lost_per_mile'] = whole_crs_proc['total_ridership_minutes_lost'] / whole_crs_proc['miles_from_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = processed['total_ridership_minutes_lost'].min()\n",
    "vmax = processed['total_ridership_minutes_lost'].max()\n",
    "color_map = LinearColormap(colors=['blue', 'yellow', 'red'], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature['properties']['total_ridership_minutes_lost']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = 'gray'  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\n",
    "        'color': color,\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7\n",
    "    }\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature['properties']\n",
    "    ridership = props['total_ridership_minutes_lost']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_name']}<br>\"\n",
    "        f\"Route: {props['route_id']}<br>\"\n",
    "        f\"Minutes Lost: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "# Define fields and aliases for tooltip (excluding 'geometry')\n",
    "tooltip_fields = [\n",
    "    'stop_id', 'stop_name', 'route_id', 'direction_id',\n",
    "    'miles_from_last', 'calculated_segment_ridership_str', 'calculated_segment_ridership',  'p50_mph_avg',\n",
    "    'diff_from_avg', 'time_lost', 'total_ridership_minutes_lost', 'uid'\n",
    "]\n",
    "\n",
    "tooltip_aliases = [\n",
    "    'Stop ID:', 'Stop Name:', 'Route ID:', 'Direction:',\n",
    "    'Miles From Last:', 'Segment Ridership:', 'Segment Ridership Aggregated:', 'Average Speed (mph):',\n",
    "    'Speed Difference:', 'Time Lost (min):', 'Total Minutes Lost:', 'uid'\n",
    "]\n",
    "\n",
    "# Add the first GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add the second GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed_one,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = '''\n",
    "    <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "# m.save('los_angeles_bus_map.html')\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Create a map centered on Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=10)\n",
    "\n",
    "# Create a color map\n",
    "vmin = processed['minutes_lost_per_mile'].min()\n",
    "vmax = processed['minutes_lost_per_mile'].max()\n",
    "color_map = LinearColormap(colors=['blue', 'yellow', 'red'], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Function to style the features\n",
    "def style_function(feature):\n",
    "    ridership = feature['properties']['minutes_lost_per_mile']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        color = 'gray'  # Use gray for null values\n",
    "    else:\n",
    "        color = color_map(ridership)\n",
    "    return {\n",
    "        'color': color,\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7\n",
    "    }\n",
    "\n",
    "# Function to create popups\n",
    "def popup_function(feature):\n",
    "    props = feature['properties']\n",
    "    ridership = props['minutes_lost_per_mile']\n",
    "    if ridership is None or np.isnan(ridership):\n",
    "        ridership_str = \"No data\"\n",
    "    else:\n",
    "        ridership_str = f\"{ridership:.2f}\"\n",
    "    return folium.Popup(\n",
    "        f\"Stop Name: {props['stop_name']}<br>\"\n",
    "        f\"Route: {props['route_id']}<br>\"\n",
    "        f\"Minutes Lost: {ridership_str}\"\n",
    "    )\n",
    "\n",
    "# Define fields and aliases for tooltip (excluding 'geometry')\n",
    "tooltip_fields = [\n",
    "    'stop_id', 'stop_name', 'route_id', 'direction_id',\n",
    "    'miles_from_last', 'calculated_segment_ridership_str', 'calculated_segment_ridership',  'p50_mph_avg',\n",
    "    'diff_from_avg', 'time_lost', 'total_ridership_minutes_lost', 'minutes_lost_per_mile'\n",
    "]\n",
    "\n",
    "tooltip_aliases = [\n",
    "    'Stop ID:', 'Stop Name:', 'Route ID:', 'Direction:',\n",
    "    'Miles From Last:', 'Segment Ridership:', 'Segment Ridership Aggregated:', 'Average Speed (mph):',\n",
    "    'Speed Difference:', 'Time Lost (min):', 'Total Minutes Lost:', 'minutes_lost_per_mile'\n",
    "]\n",
    "\n",
    "# Add the first GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add the second GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    processed_one,\n",
    "    style_function=style_function,\n",
    "    popup=popup_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=tooltip_fields,\n",
    "        aliases=tooltip_aliases,\n",
    "        localize=True,\n",
    "        sticky=False,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: #F0EFEF;\n",
    "            border: 2px solid black;\n",
    "            border-radius: 3px;\n",
    "            box-shadow: 3px;\n",
    "            padding: 10px;\n",
    "            font-size: 12px;\n",
    "        \"\"\",\n",
    "        max_width=800,\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add the color map to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Add a title to the map\n",
    "title_html = '''\n",
    "    <h3 align=\"center\" style=\"font-size:16px\"><b>Los Angeles Bus Segments</b></h3>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map\n",
    "#m.save('los_angeles_bus_map.html')\n",
    "\n",
    "# Display the map (if you're in a Jupyter notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_one.sort_values('total_ridership_minutes_lost', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total ridership minutes lost across all segments\n",
    "total_minutes = processed['total_ridership_minutes_lost'].sum() + processed_one['total_ridership_minutes_lost'].sum()\n",
    "\n",
    "# Convert to hours for better readability\n",
    "total_hours = total_minutes / 60\n",
    "\n",
    "per_pass = total_minutes / 770000\n",
    "\n",
    "print(f\"Total passenger minutes lost: {total_minutes:,.2f}\")\n",
    "print(f\"Total passenger hours lost: {total_hours:,.2f}\")\n",
    "print(f\"Total minutes per passenger lost: {per_pass:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both to WGS 84\n",
    "processed = processed.to_crs(\"EPSG:4326\")\n",
    "processed_one = processed_one.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Now concatenate\n",
    "recombined = pd.concat([processed, processed_one], ignore_index=True)\n",
    "\n",
    "# Export to GeoJSON\n",
    "recombined.to_file(\"combined_routes.geojson\", driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
